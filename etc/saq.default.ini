[global]

;           _____ ______ 
;     /\   / ____|  ____|
;    /  \ | |    | |__   
;   / /\ \| |    |  __|  
;  / ____ \ |____| |____ 
; /_/    \_\_____|______|
;                        
; Analysis Correlation Engine

;
; DEFAULT CONFIGURATION
; *** DO NOT EDIT THIS FILE UNLESS YOU ARE A DEVELOPER ***
; override this configuration by creating the file etc/saq.ini with the local configuration settings 
;

; Unless otherwise indicated, all file and directory paths should be relative to the 
; SAQ_HOME path (the directory where ACE is installed)

; the default company name and id to use for alerts generated by engines using this configuration file
; SELECT * FROM company
company_name = OVERRIDE
company_id = OVERRIDE
; shows up in the GUI so you know which instance you're logged into
instance_name = OVERRIDE
; the name of this node, usually this is the fully qualified domain name
; if this is set to AUTO then socket.getfqdn() is used
node = OVERRIDE
; this can be either PRODUCTION, QA or DEV
; used by some routines to prevent a user from making a horrible mistake
; if this setting is missing then the default value taken is PRODUCTION
instance_type = OVERRIDE
; forces the entire application to run under a single thread
debug = no
; global data directory that has all the things (relative to SAQ_HOME)
data_dir = data
; temp directory for various whatever (relative to DATA_DIR)
tmp_dir = var/tmp
; directory that contains any stack traces for review (relative to DATA_DIR)
error_reporting_dir = error_reports
; list of email addresses (comma separated) that get these reports sent to them automatically
; you can leave this empty
error_reporting_email = OVERRIDE

; the amount of time (in minutes) in between reporting of the same condition
; set to 0 to disable condition reporting
condition_reporting_delay = 480

; comma separated list of local domains
local_domains = OVERRIDE

; set to yes to log all SQL commands executed by the server
log_sql = no

; set to yes to log all SQL commands and their execution time
log_sql_exec_times = no

; set this to True to enable semaphores
; you'll definitely want this to be True in production settings
enable_semaphores = yes

; set this to the total number of analysis engines (of any type) you have running
global_engine_instance_count = 1

; the number of days alerts set to IGNORE will last until they are deleted from the system
ignore_days = 1

; the number of days alerts set to FALSE_POSITIVE will last until they are reset
fp_days = 30

; when alerts are being processed they are locked for processing
; if a process dies during process then a stale lock could linger
; the amount of time a lock is considered valid (in MM:SS format)
lock_timeout = 240:00

; amount of time (in seconds) between lock keep alive messages
; or "how often do I update the lock_time field of the locks database while I've got the lock open?"
lock_keepalive_frequency = 10

; amount of time (in seconds) that we expect analysis to take, in general
; we use this to warn ourselves that something might be wrong with logic in a module
maximum_cumulative_analysis_warning_time = 120

; amount of time (in seconds) to give analysis (in total) before we bail entirely
maximum_cumulative_analysis_fail_time = 900

; amount of time (in seconds) that we expect a single analysis module to take
maximum_analysis_time = 60

; amount of time (in seconds) that you expect to wait for a threaded analysis module to finish up
; this is meant to catch poorly written threaded analysis modules
execution_thread_long_timeout = 30

; how often (in seconds) modules check to see if watched files have been modified
check_watched_files_frequency = 5

; issue a warning when a worker process uses this much memory (in MB)
memory_limit_warning = 512

; kill a worker when it uses this much memory (in MB)
memory_limit_kill = 1024

[collection]
; contains various persistant information used by collectors (relative to DATA_DIR)
persistence_dir = var/collection/persistence
; some a submission is presented to the saq.collectors.Collector it moves
; all the files attached to the submission into this directory
; (relative to DATA_DIR)
incoming_dir = var/collection/incoming

[node_translation]
; when ACE looks up a node to send something to, it does so using the nodes.location from the ace database
; this value is populated by the node itself, typically using the name of the system (or via configuration file)
; in some situations you may need to map this to another value
; for example, if the target node is behind a NAT on another host, you can remap it here
; the format is key = source_host:source_port,target_host:target_port
; where key is a unique name for the mapping
; source_host:source_port is the value in the database
; and target_host:target_port is the actual value that needs to be used for this system

[SSL]
ca_chain_path = ssl/ca-chain.cert.pem

[proxy]
; leave these values blank if you do not need to use a proxy
host = OVERRIDE
port = OVERRIDE
user = OVERRIDE
password = OVERRIDE


; additional proxy settings can be added and referred to by analysis modules that might use more than one
; for example, the crawlphish analysis module can use multiple proxies
; additional proxies have the format [proxy_name] where name is a unique name for the proxy

;[proxy_tor]
; tor socks proxy configuration
; leave these blank if you're not using tor
;host = 
;port =
;user =
;password =

[api]
; this is the single threaded developer server API config
; production used apache configuration (see etc/ace_api_apache.conf)
; binding address to incoming requests
listen_address = 0.0.0.0
listen_port = 443
; ssl parameter (required)
ssl_cert = OVERRIDE
ssl_key = OVERRIDE
secret_key = OVERRIDE

; the default alert type when submissions do not include it
default_alert_type = default

; the prefix that other systems use to connect to the API server for this system
; https://PREFIX/api/analysis/etc...
; if set to AUTO then the prefix will be simply socket.getfqdn() (which defaults to port 443)
prefix = AUTO

[gui]
; this is the single threaded developer server GUI config
; production used apache configuration (see etc/saq_apache.conf)
; binding address to incoming requests
listen_address = 0.0.0.0
listen_port = 5000
; ssl parameter (required)
ssl_cert = OVERRIDE
ssl_key = OVERRIDE

; force users to authenticate to the gui, else everyone get's access
authentication = yes
; which tabs should be displayed on the gui
display_events = yes
display_metrics = yes
display_overview = yes

; Google Analytics for Public ACE
; if true, expects $SAQ_HOME/app/templates/gtag.html to exist
google_analytics = off

; alert management features access through the GUI
dispositioning = on
ownership = on
email_remediation = on
event_management = on
add_observable = on
analyze_alert = on
fp_easy_button = on
ignore_easy_button = on
upload_vt = on
upload_vxstream = on
view_in_vx = on

; Hide intel if we don't want to let people see it (public/internet ace)
hide_intel = no

; the base address for the GUI (used to build ACE permalinks)
base_uri = OVERRIDE

; default company for manual analysis (the default option from the drop down)
; 3 == integral
default_company_id = OVERRIDE

; define what companies are "core" companies
; these are the company IDs separated by commas
core_companies = OVERRIDE

; secret key used by flask
secret_key = OVERRIDE

;
; define the global SLA settings
; see below to configure SLA for a specific company or alert type
;

[SLA]
; set this to no to disable SLA globally (useful for debug/qa systems)
enabled = no
; the following values are the global defaults
; the amount of time (in hours) to SLA
time_to_dispo = 8
; the amount of time (in hours) before SLA is exceeded to warn
approaching_warn = 1
; comma separated list of alert types that are excluded from SLA
excluded_alert_types = 
; Specify the start and end business hour. 24 hour clock. Seperated by a comma
; -- default: 6,18 for start_hour=0600 and end_hour=1800 (6pm)
business_hours = 6,18
; The time_zone should be a key in pytz.all_timezones. This field tells us what
; time zone business_hours are in (0600 UTC is different from 0600 Eastern) 
time_zone = US/Eastern

; additional settings can be defined for specific alerts
; the format is as follows
; [SLA_unique_name] where _unique_name can be any unique descriptive name
; property = the property of the alert to match
; value = the value of the property to match
; these settings can be applied
; enabled =
; time_to_dispo =
; approaching_warn =

[network_semaphore]
; the address of the network semaphore server (used to bind and listen)
bind_address = OVERRIDE
bind_port = 53559

; the address of the network semaphore server to the clients that want to use them
; could be the same as the bind_adress and bind_port above
remote_address = OVERRIDE
remote_port = 53559

; comma separated list of source IP addresses that are allowed to connect
allowed_ipv4 = 127.0.0.1
; directory that contains metrics and current status of semaphores
stats_dir = var/network_semaphore

; SEMAPHORE CAPACITY LIMITS
;
; each of these represents a resource and the number of simultaneous connections allowed to it
semaphore_pcap = 1
semaphore_splunk = 1
semaphore_carbon_black = 1

[network_configuration]
; this section defines what the managed network looks like
;
; command separated list of CIDR notation for managed networks
managed_networks = 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12

[mediawiki]
domain = https://wiki.local/
; main url prefix for mediawiki
uri = https://wiki.local/display/integral/AlertContext
; url suffix for alert documentation
alert_suffix = #AlertContext-

[smtp]
; smtp information for sending emails
server = OVERRIDE
mail_from = OVERRIDE

; deprecated config setting
remediation_address = 

[remediation]
; EWS remediation host and port
ews_host = OVERRIDE
ews_port = 3100

; comma separated list of email addresses to ignore when performing remediation
; these are typically functional email addresses, such as those used to journal
excluded_emails=

[email_archive]
; if this system is archving emails, this determines what section to use for the database config
; NOTE this is the name of the section without the leading database_ (legacy issue)
primary = email_archive

[memcached]
; the address of the memcached system used by ACE
client_address = unix:var/memcached.socket

[bro]
; the directory that contains the HTTP streams generated by bro/ace_http.bro (relative to DATA_DIR)
http_dir = var/bro/http
; the directory that contains the SMTP streams generated by bro/ace_smtp.bro (relative to DATA_DIR)
smtp_dir = var/bro/smtp

[email]
; the directory that contains the emails received by ACE for scanning
; either via postfix or local email routing (not SMTP stream scanning)
; (relative to DATA_DIR)
email_dir = var/incoming/amc
; the strftime format used to generate the subdirectory names that actually contain the emails
; NOTE that you have to use two %% to escape in this INI format
subdir_format = %%Y%%m%%d%%H

#
# unix_socket is typically /var/run/mysqld/mysqld.sock
#

[database_ace]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE
;ssl_key = ssl/mysql/client-key.pem
;ssl_cert = ssl/mysql/client-cert.pem
;ssl_ca = ssl/mysql/ca-cert.pem

[database_brocess]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE
; how long do we wait for brocess queries to complete (in seconds)
; these queries should complete super fast
; failure to complete the timeout will send the analysis module using brocess into cooldown mode
query_timeout = 5

[database_hal9000]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_email_archive]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_vt_hash_cache]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[ldap]
enabled = yes
tivoli_enabled = no
; the LDAP server (probably your domain controller)
ldap_server = OVERRIDE
tivoli_server = 
; the port the LDAP server listens to (defaults to 389)
ldap_port = 389
tivoli_ldap_port = 389
; user account to use for authentication
ldap_bind_user = OVERRIDE
tivoli_bind_user = 
; enter the LDAP password, or PROMPT to force the system to prompt
;ldap_bind_password = PROMPT
ldap_bind_password = OVERRIDE
tivoli_bind_password = 
; the base DN for searching
ldap_base_dn = OVERRIDE
tivoli_base_dn = 

[splunk]
enabled = yes
; the splunk query server
uri = OVERRIDE
; user account information for splunk
username = OVERRIDE
password = OVERRIDE
; the maximum number of results a single query can generate
max_result_count = 100
; the relative duration of time to search for around the event
relative_duration_before = 00:15:00
relative_duration_after = 00:00:01

[elk]
enabled = no
; the URI used for queries (format hostname[:port]/path)
uri =
; the default cluster to search
; leave this value empty to not specify the cluster
; use * to search all clusters
cluster = 
; user account information for elk (with search guard)
; leave this blank if you're not using search guard
username =
password = 

; the maximum number of results a single query can generate
max_result_count = 100
; the default relative duration of time to search if not specified
relative_duration_before = 00:15:00
relative_duration_after = 00:00:01

[splunk_logging]
; location of generated splunk logs (relative to DATA_DIR)
splunk_log_dir = splunk_logs

[elk_logging]
; the base directory for all log files destined for elasticsearch (relative to DATA_DIR)
elk_log_dir = es_logs

[crits]
url = OVERRIDE
mongodb_uri = OVERRIDE
api_key = OVERRIDE
activity_url = OVERRIDE
indicators_endpoint = /api/v1/indicators/
username = 
; path to sqlite database cache of crits indicators (relative to DATA_DIR)
cache_db_path = var/crits.db

[crits_yara_export]
; settings for exporting crits indicators into yara rules
; a comma separated list of crits indicator types to export to yara rules
export_list = ipv4_address, email_address, email_subject, email_x_mailer, hex_string, domain, user_agent, uri, file_name, file_path, mutex, registry_key, email_header_field, email_x_originating_ip

; a comma separated list of sources to include (defaults to all sources if left empty)
export_sources_include =
; a comma separated list of sources to exclude (defaults to none if left empty)
export_sources_exclude =
; directory of template files
export_template_dir = etc/yara_templates
; minimum length of an indicator value that would go into a yara rule
export_minimum_length = 5

[crits_yara_export_string_modifiers]
; specifies what string modifiers to use per indicator type
; see https://yara.readthedocs.io/en/v3.7.0/writingrules.html#text-strings
; if an indicator type has no mapping then the default setting is used
default = ascii wide nocase
domain = ascii wide nocase fullword
file_name = ascii wide nocase fullword

[crits_indicator_type_mapping]

; indicator type mapping
; if your crits installation has different names for indicator types you can set those here
; otherwise it uses the default crits indicator names
; these were pulled from https://github.com/crits/crits/blob/master/crits/vocabulary/indicators.py on 10/16/2018

adjust_token = Adjust Token
api_key = API Key
as_number = AS Number
as_name = AS Name
bank_account = Bank account
bitcoin_account = Bitcoin account
certificate_fingerprint = Certificate Fingerprint
certificate_name = Certificate Name
checksum_crc16 = Checksum CRC16
cmd_line = Command Line
company_name = Company name
cookie_name = Cookie Name
country = Country
crx = CRX
debug_path = Debug Path
debug_string = Debug String
dest_port = Destination Port
device_io = Device IO
doc_from_url = Document from URL
domain = Domain
email_boundary = Email Boundary
email_address = Email Address
email_from = Email Address From
email_header_field = Email Header Field
email_helo = Email HELO
email_message_id = Email Message ID
email_originating_ip = Email Originating IP
email_reply_to = Email Reply-To
email_sender = Email Address Sender
email_subject = Email Subject
email_x_mailer = Email X-Mailer
email_x_originating_ip = Email X-Originating IP
file_created = File Created
file_deleted = File Deleted
file_moved = File Moved
file_name = File Name
file_opened = File Opened
file_path = File Path
file_read = File Read
file_written = File Written
get_param = GET Parameter
hex_string = HEX String
html_id = HTML ID
http_request = HTTP Request
http_resp_code = HTTP Response Code
imphash = IMPHASH
ipv4_address = IPv4 Address
ipv4_subnet = IPv4 Subnet
ipv6_address = IPv6 Address
ipv6_subnet = IPv6 Subnet
latitude = Latitude
launch_agent = Launch Agent
location = Location
longitude = Longitude
mac_address = MAC Address
malware_name = Malware Name
md5 = MD5
memory_alloc = Memory Alloc
memory_protect = Memory Protect
memory_read = Memory Read
memory_written = Memory Written
mutant_created = Mutant Created
mutex = Mutex
name_server = Name Server
other_file_op = Other File Operation
password = Password
password_salt = Password Salt
payload_data = Payload Data
payload_type = Payload Type
pipe = Pipe
post_data = POST Data
process_name = Process Name
protocol = Protocol
referer = Referer
referer_of_referer = Referer of Referer
registrar = Registrar
registry_key = Registry Key
reg_key_created = Registry Key Created
reg_key_deleted = Registry Key Deleted
reg_key_enumerated = Registry Key Enumerated
reg_key_monitored = Registry Key Monitored
reg_key_opened = Registry Key Opened
reg_key_value_created = Registry Key Value Created
reg_key_value_deleted = Registry Key Value Deleted
reg_key_value_modified = Registry Key Value Modified
reg_key_value_queried = Registry Key Value Queried
service_name = Service Name
sha1 = SHA1
sha256 = SHA256
sms_origin = SMS Origin
source_port = Source Port
ssdeep = SSDEEP
telephone = Telephone
time_created = Time Created
time_updated = Time Updated
tracking_id = Tracking ID
ts_end = TS End
ts_start = TS Start
uri = URI
uri_path = URI Path
user_agent = User Agent
user_id = User ID
victim_ip = Victim IP
volume_queried = Volume Queried
webstorage_key = Webstorage Key
web_payload = Web Payload
whois_name = WHOIS Name
whois_addr1 = WHOIS Address 1
whois_addr2 = WHOIS Address 2
whois_registrant_email_address = WHOIS Registrant Email Address
whois_telephone = WHOIS Telephone
xpi = XPI

[crits_observable_type_mappping]
; observable type mapping to indicator types
cidr =
ipv4 = ipv4_address
ipv4_conversation =
fqdn = domain
hostname = domain
http_request = 
asset = ipv4_address
user = user_id
url = uri
pcap = 
file = 
file_path = file_path
file_name = file_name
file_location =
email_address = email_address
email_conversation = 
yara =
yara_rule =
indicator =
md5 = md5
sha1 = sha1
sha256 = sha256
snort_sig = 
message_id = email_message_id
process_guid =

[yara]
; base directory where the yara scanner server (yss) runs out of (relative to SAQ_HOME)
yss_base_dir = yss
; relative directory where the unix sockets for the yara scanner server are located
yss_socket_dir = socket
; global configuration of yara rules
; format is signature_(repo|dir|file)_unique_identifier
; "repo" is a directory that is a git repository
; "dir" is a directory that contains .yar files
; "file" is a single yara file
; the value is the path to the directory or file
; signature_repo_custom = etc/yara/custom
; the blacklist contains a list of rule names (one per line) to exclude from the results
blacklist_path = etc/yara.blacklist
; a directory that contains all the files that fail to scan (relative to DATA_DIR)
scan_failure_dir = scan_failures

[virus_total]
; virus total authentication
api_key = 
; lookup URL
query_url = https://www.virustotal.com/vtapi/v2/file/report
; download URL
download_url= https://www.virustotal.com/vtapi/v2/file/download
; storage directory for downloaded VT files (relative to installation dir)
cache_dir = vt_cache


[vxstream]
; the base URI for all vxstream requests (should be your local vxstream installation)
baseuri = OVERRIDE
baseuri_v2 = OVERRIDE
; the environment to submit all samples into
environmentid = OVERRIDE
; authentication stuff
apikey = OVERRIDE
secret = OVERRIDE
; the baseuri to use from the GUI
gui_baseuri = OVERRIDE

[carbon_black]
; carbon black server API location and authentication
url = OVERRIDE
token = 
; site specific credential file for new CBapi 
credential_file = etc/carbon_black.auth
; cbinterface.modules.process.events_to_json essentially re-creates the entire CB
; process document. This segment_limit var specifies the limit of process segments to store.
; If not set, some extreamly large processes will cause analysis problems.
; Additionally, most malicious processes don't produce a lot of segments.
; Note: analysis.details for this module contains the count of processed segments,
; as well as, the count of total process segments.
segment_limit = 10

; the url displayed and used in the GUI for analysts
; this may be different than url
gui_url = OVERRIDE

; defines what we do with the malicious files we find
[malicious_files]
; this is where we actually store copies of the files
malicious_dir = malicious
; comma separated list of email accounts to send notifications to when new files are added
malicious_alert_recipients = 

[chronos]
; the amount of time (in seconds as a float) in between status checks for requested locks
status_check_frequency = 1.0
; location of the chronos system
host = https://localhost
port = 5032
path = chronos/
; location of the chronos system
analysis-host = analysis.local
analysis-port = 5031

; define groups of observables 
; all sections must begin with observable_group_
; and then any number of define_UNIQUE_ID = type:value
;
; these can then be referenced in module definitions by using the following specifier
; exclude_group_01 = observable_group:group_name
; where group_name is the text after observable_group_
;
; for example, say you define a group called observable_group_internal
; to exclude everything in this group from an analysis module, add the following to the module config
; exclude_blah_01 = observable_group:internal

[observable_group_internal]
; all internal IP addresses
define_internal_04 = ipv4:10.0.0.0/8
define_internal_05 = ipv4:192.168.0.0/16
define_internal_06 = ipv4:172.16.0.0/12

[observable_group_proxy]
; all known proxy nodes (bluecoat and squid)

[observable_group_smtp]
; SMTP related services

[observable_group_external_gateway]
; exit nodes for outbound traffic

[observable_group_internal_dns]
; internal BIND and AD servers for DNS

[observable_group_external_dns]
; external BIND servers for DNS

[observable_group_popular_fqdn]
define_fqdn_google = fqdn:google.com
define_fqdn_amazon = fqdn:amazon.com
define_fqdn_yahoo = fqdn:yahoo.com
define_fqdn_youtube = fqdn:youtube.com

; global observable exclusions
; things we never want to analyze because they are noisy, errors or stupid
[observable_exclusions]
; format is random_name = observable_type:observable_value

; ignore blank user IDs
exclude_user = user:-
exclude_user_unknown = user:unknown
exclude_user_system = user:system
; loopback
exclude_loopback = ipv4:127.0.0.1
; weird
exclude_invalid_ipv4 = ipv4:0.0.0.0

; office365 journaling

; super common websites
exclude_google = fqdn:google.com
exclude_youtube = fqdn:youtube.com

[deprecated_modules]
; a list of analysis modules that have been removed from the system but are still references in the data.json files
analysis_module_dlp_baseline = saq.modules.dlp:DLPBaselineAnalysis

;
; CUSTOM ALERTS
;

[custom_alerts]
debug = analysis/custom/saq_debug.html
brotex - smtp = analysis/custom/brotex_smtp.html
brotex - smtp - v2 = analysis/custom/brotex_smtp_v2.html
mailbox = analysis/custom/email_alert.html
o365 = analysis/custom/email_alert.html
brotex - http = analysis/custom/brotex_http.html
splunk - snort = analysis/custom/splunk_snort.html
splunk - av = analysis/custom/splunk_av.html
splunk - bit9 = analysis/custom/splunk_bit9.html
splunk - cb = analysis/custom/cb.html
carbonblack - watchlist = analysis/custom/cb_watchlist.html
cloudphish = analysis/custom/cloudphish.html
http = analysis/custom/http.html
eventsentry = analysis/custom/eventsentry.html

;
; ANALYSIS MODULES
;

; configuration format is as follows
; [analysis_module_UNIQUE_MODULE_NAME] <-- must begin with analysis_module_ and module_name must be unique
; module = MODULE_PATH <-- the module path the AnalysisModule classes is located in
; class =  MODULE_CLASS
; enabled = yes | no
; module_groups = group1,group2,...
; exclude_UNIQUE_NAME = type:value
; expect_UNIQUE_NAME = type:value
;
; optional parameters
; priority = int (lower priority goes first, defaults to 0 (equal priority to all))
; threaded = yes | no
; threaded_execution_frequency = int
;
; you basically want everything enabled except for the things that are broken or deprecated
; you control what runs per engine configuration
;

;[analysis_module_detection]
;module = saq.modules.alerts
;class = ACEDetectionAnalyzer
;enabled = yes

; the analysis_mode that gets set when one or more detections are found during analysis
;target_mode = correlation

;[analysis_module_alert_disposition_analyzer]
;module = saq.modules.alerts
;class = ACEAlertDispositionAnalyzer
;enabled = yes
;threaded = yes
;threaded_execution_frequency = 5
;;target_mode = correlation

[analysis_module_mailbox_email_analyzer]
module = saq.modules.email
class = MailboxEmailAnalyzer
enabled = yes

[analysis_module_bro_smtp_analyzer]
module = saq.modules.email
class = BroSMTPStreamAnalyzer
enabled = yes

[analysis_module_bro_http_analyzer]
module = saq.modules.http
class = BroHTTPStreamAnalyzer
enabled = yes

; path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist

[analysis_module_email_analyzer]
module = saq.modules.email
class = EmailAnalyzer
enabled = yes

; relative path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist

; office365 journaling will cause outbound emails to also get journaled
; set this to no to scan outbound office365 emails
scan_inbound_only = yes

[analysis_module_email_link_analyzer]
module = saq.modules.advanced
class = EmailLinkAnalyzer
enabled = yes

[analysis_module_email_archiver]
module = saq.modules.email
class = EmailArchiveAction
enabled = no

; the directory to contain the archived emails (relative to DATA_DIR)
archive_dir = archive/email
; how long to keep archived emails (in days)
expiration_days = 7

[analysis_module_encrypted_archive_analyzer]
module = saq.modules.email
class = EncryptedArchiveAnalyzer
enabled = yes

[analysis_module_file_path_analysis]
module = saq.modules.file_path
class = FilePathAnalyzer
enabled = yes

[analysis_module_hal9000]
module = saq.modules.hal9000
class = HAL9000Analyzer
enabled = yes

; this module needs to run last since it makes it's decision on if an alert was generated or not
priority = 100

exclude_proxy = observable_group:proxy
exclude_smtp = observable_group:smtp
exclude_external_gateway = observable_group:external_gateway
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns

; minimum sample size required for tagging
min_sample_size = 30
; percent threshold ABOVE which the observable is tagged as high_mal_frequency
mal_threshold = 90
; percent threshold BELOW which the observable is tagged as high_fp_frequency
fp_threshold = 10

[analysis_module_carbon_black_process_analysis_v1]
module = saq.modules.carbon_black
class = CarbonBlackProcessAnalyzer_v1
enabled = yes
semaphore = carbon_black
max_results = 1

[analysis_module_process_guid_analysis]
module = saq.modules.process
class = ProcessGUIDAnalyzer
enabled = yes
semaphore = carbon_black

[analysis_module_carbon_black_netconn_source_analysis]
module = saq.modules.carbon_black
class = CarbonBlackNetconnSourceAnalyzer
enabled = yes
semaphore = splunk
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00
observation_grouping_time_range = 45:00

; the maximum number of process guids to extract
process_guid_limit = 3

[analysis_module_dlp_process_hash_analysis_v1]
module = saq.modules.process
class = DLPProcessHashAnalyzer_v1
enabled = yes
semaphore = splunk
max_asset_count = 6
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_bit9_file_hash_analysis_v1]
module = saq.modules.process
class = Bit9FileHashAnalyzer_v1
enabled = no
semaphore = splunk
max_asset_count = 6
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_vt_hash_analyzer]
module = saq.modules.vt
class = VTHashAnalyzer
enabled = yes

; an (optional) comma separated list of vendors to ignore in VT results
ignored_vendors = Tencent,Cylance,eGambit,Endgame,Zillya,Trapmine
; vt_hash_cache url
query_url = OVERRIDE
use_proxy = no

[analysis_module_vt_hash_downloader]
module = saq.modules.vt
class = VTHashFileDownloader
enabled = yes

[analysis_module_alert_correlation]
module = saq.modules.alerts
class = ACEAlertsAnalyzer
enabled = yes

[analysis_module_pcap_extraction]
module = saq.modules.pcap
class = PcapExtraction
enabled = no
semaphore = pcap
; relative time capture (see -d option of the extract script)
relative_duration = 00:01:00
; pcap executable path
executable_path = bin/extract
; custom pcap extraction configuration path for ACE
config_path = etc/pcap_extract.ini
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 2

[analysis_module_pcap_conversation_extraction]
module = saq.modules.pcap
class = PcapConversationExtraction
enabled = yes
semaphore = pcap
; relative time capture (see -d option of the extract script)
relative_duration = 00:01:00
; pcap executable path
executable_path = bin/extract
; custom pcap extraction configuration path for ACE
;
config_path = etc/pcap_extract.ini
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 2

[analysis_module_network_identifier]
module = saq.modules.asset
class = NetworkIdentifier
enabled = yes
; the CSV file that contains the definitions
csv_file = etc/local_networks.csv

[analysis_module_asset_analyzer]
module = saq.modules.asset
class = AssetAnalyzer
enabled = no

[analysis_module_netbios_analyzer]
module = saq.modules.asset
class = NetBIOSAnalyzer
enabled = yes
; if this is set then the command is prefixed with ssh ssh_host so that it executes from another system
; use this if you are in AWS and your target is inside target network
ssh_host = 

[analysis_module_dns_analyzer]
module = saq.modules.asset
class = DNSAnalyzer
enabled = no
; comma separated list of domains you want to analyze
; this will probably mirror your search list in /etc/resolv.conf
local_domains = 

; if this is set then the command is prefixed with ssh ssh_host so that it executes from another system
; use this if you are in AWS and your target is inside target network
ssh_host = 

[analysis_module_active_directory_analyzer]
module = saq.modules.asset
class = ActiveDirectoryAnalyzer
enabled = yes

[analysis_module_snort]
module = saq.modules.snort
class = SnortAlertsAnalyzer
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
; tighter time around splunk searches
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_elk_snort]
module = saq.modules.elk
class = SnortAlertsAnalyzer
enabled = yes
semaphore = elk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_snort_signature_analysis_v1]
; also see bin/update_snort_rules
module = saq.modules.snort
class = SnortSignatureAnalyzer_v1
enabled = yes
; relative path to snort rules
rules_dir = etc/snort

[analysis_module_pan_threats]
module = saq.modules.pan
class = PanThreatsAnalyzer
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
; tighter time around splunk searches
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_pan_snort_correlation]
module = saq.modules.pan
class = PanSnortCorrelationAnalyzer
enabled = yes
semaphore = splunk
relative_duration_before = 00:05:00
relative_duration_after = 00:05:00

[analysis_module_dns_request_analysis_v1]
module = saq.modules.dns
class = DNSRequestAnalyzer_v1
enabled = yes
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 50
; if there are less than X users requesting the resource in the timeframe then we add the users as observables
max_source_count = 6
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00
; 24 hour baseline period
baseline_relative_duration_before = 24:00:00
baseline_relative_duration_after = 02:00:00

[analysis_module_bluecoat_analysis_by_dst_v1]
module = saq.modules.bluecoat
class = BluecoatProxyAnalyzerByDestination_v1
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:internal
; the maximum number of proxy requests to obtain from splunk
max_request_count = 50
; if there are less than X users requesting the resource in the timeframe then we add the users as observables
max_user_count = 6
; tighter time around splunk searches
relative_duration_before = 00:15:00
relative_duration_after = 00:15:00
; 24 hour baseline period
baseline_relative_duration_before = 24:00:00
baseline_relative_duration_after = 02:00:00
; a CSV file that maps bluecoat categories to tags
category_tag_csv_path = etc/bluecoat_category_tagging.csv

[analysis_module_bluecoat_analysis_by_src_v1]
module = saq.modules.bluecoat
class = BluecoatProxyAnalyzerBySource_v1
enabled = yes
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 1000
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00

[analysis_module_squid]
module = saq.modules.squid
class = SquidProxyAnalyzerByDestination
enabled = no
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 10
; tighter time around splunk searches
relative_duration_before = 00:15:00
relative_duration_after = 00:15:00

[analysis_module_exploit_kit_proxy_analyzer]
module = saq.modules.bluecoat
class = ExploitKitProxyAnalyzer
enabled = yes
semaphore = splunk
exclude_internal = observable_group:internal
; the maximum number of proxy requests to obtain from splunk
max_request_count = 10
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00

[analysis_module_symantec]
module = saq.modules.symantec
class = SymantecAnalyzer
enabled = yes
semaphore = splunk
; tighter time around splunk searches
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_dlp_process]
module = saq.modules.dlp
class = DLPProcessAnalyzer
enabled = yes
semaphore = splunk
; we go a bit wider for DLP Process searches
relative_duration_before = 24:00:00
relative_duration_after = 02:00:00
; go pretty far back for the baseline
baseline_relative_duration_before = 720:00:00
baseline_relative_duration_after = 02:00:00
; this one takes a long time to run so don't go back too far (7 days should be good)
global_baseline_relative_duration_before = 168:00:00
global_baseline_relative_duration_after = 02:00:00

[analysis_module_user_analyzer]
module = saq.modules.user
class = UserAnalyzer
enabled = yes

[analysis_module_email_conversation_frequency_analyzer]
module = saq.modules.email
class = EmailConversationFrequencyAnalyzer
enabled = yes
cooldown_period = 60
; when two people email each other frequently we want to know that
; this is the minimum number of times we've seen this email address email this other email address
; that we consider to be "frequent"
conversation_count_threshold = 5

[analysis_module_email_conversation_attachment_analyzer]
module = saq.modules.email
class = EmailConversationAttachmentAnalyzer
enabled = yes

[analysis_module_email_address_analyzer]
module = saq.modules.user
class = EmailAddressAnalyzer
enabled = yes

[analysis_module_url_email_pivot_analyzer]
module = saq.modules.email
class = URLEmailPivotAnalyzer
enabled = yes
; the maximum amount of results to download
; if there are more emails, only the count is recorded
result_limit = 35

[analysis_module_cloudphish_url_email_pivot_analyzer]
module = saq.modules.email
class = CloudphishURLEmailPivotAnalyzer
enabled = yes

[analysis_module_email_history_analyzer_v2]
module = saq.modules.email
class = EmailHistoryAnalyzer_v2
enabled = yes
semaphore = splunk

; we go back a bit to try to catch phish sitting idle in the inbox
relative_duration_before = 72:00:00
relative_duration_after = 02:00:00

; the following is a list of comma-separated domains that are aliased together
; so if a user's email address domain matches on of these then the entire group is searched with "OR" clause
; for example, john@ashland.com would search for john@teamashland.onmicrosoft.com OR john@ashland.com
; each of these configuration items must start with map_ at the beginning of the name
;map_company_1 = teamcompany.onmicrosoft.com,company.com

[analysis_module_email_logger]
module = saq.modules.email
class = EmailLoggingAnalyzer
enabled = yes

; set this to yes to enable log formatted for splunk
splunk_log_enabled = yes

; the subdirectory inside of splunk_log_dir (see [splunk_logging]) that contains the logs
splunk_log_subdir = smtp
; set this to yes to update the smtplog table of the brocess database
update_brocess = yes

; elasticsearch JSON logging
json_log_enabled = yes

; relative file path of the generated JSON logs
; the file name is passed through strftime 
; https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior
; {pid} is replaced with the process ID of the current executing process 
; so that we don't have multiple processes writing to the same file
json_log_path_format = email/email-{pid}-%%Y%%m%%d.json

[analysis_module_crits_analyzer]
module = saq.modules.crits
class = CritsAnalyzer
enabled = yes

[analysis_module_crits_observable_analyzer]
module = saq.modules.crits
class = CritsObservableAnalyzer
enabled = yes
exclude_common = observable_group:popular_fqdn

[analysis_module_faqueue_alert_analyzer]
module = saq.modules.crits
class = FAQueueAlertAnalyzer
enabled = yes

[analysis_module_vpn_analyzer]
module = saq.modules.vpn_analysis
class = VPNAnalyzer
enabled = yes
semaphore = splunk

; we need to look way back and after to see when they logged in and off
relative_duration_before = 24:00:00
relative_duration_after = 24:00:00

[analysis_module_file_hash_analyzer]
module = saq.modules.file_analysis
class = FileHashAnalyzer
enabled = yes
; ignored file patterns (glob style pattern matching)
ignore_pattern_pcap = *.pcap
ignore_pattern_tshark = *.tshark
ignore_pattern_brotex_1 = *.brotex.tar
ignore_pattern_brotex_2 = *.brotex.tar.gz
ignore_pattern_stderr = *.stderr
ignore_pattern_stdout = *.stdout
ignore_pattern_pdfparser = *.pdfparser
ignore_pattern_stream = *.stream
ignore_pattern_noxml = *.noxml
ignore_pattern_ole = */ole.stdout.analysis
ignore_pattern_officeparser = *.officeparser/*
; not sure why these were in there
;ignore_pattern_olevba = *.olevba/*
;ignore_pattern_vtdownload = */vt_downloads/*
ignore_pattern_brotex_smtp_protocol = */protocol.smtp
ignore_pattern_brotex_http_protocol = */protocol.http
ignore_pattern_brotex_uri_file = */smtp.uriurl.crits
ignore_pattern_brotex_domain_file = */smtp.uridomainname.crits
ignore_pattern_brotex_connection = */connection
;ignore_pattern_email = */email.rfc822
ignore_pattern_email_headers = */email.rfc822.headers
ignore_pattern_email_plain_text = */email.rfc822.unknown_text_plain_*
ignore_mime_type_xml = application/xml
ignore_mime_type_text_xml = text/xml
ignore_mime_type_html = text/html
ignore_mime_type_images = image/*
ignore_mime_type_email = message/rfc822
ignore_pattern_wildfire_reports =  *.wildfire/report.*
ignore_pattern_json = *.json
ignore_pattern_extra_001 = *.crl
ignore_pattern_extra_002 = */xl/drawings/*
ignore_pattern_extra_003 = */xl/media/*
ignore_pattern_extra_004 = */xl/printerSettings/*
ignore_pattern_extra_005 = */xl/theme/*
ignore_pattern_extra_006 = *.rels
ignore_pattern_extra_007 = */xl/vbaProject.bin
ignore_pattern_pdf_txt = *.pdf_txt
ignore_pattern_olevba = *.olevba/*.bas
ignore_pattern_pcode = *.pcode.bas

[analysis_module_correlated_tag_analyzer]
module = saq.modules.tag
class = CorrelatedTagAnalyzer
enabled = no

; powerpoint autoexec on mouse over to powershell
definition_001_rule = ppt_mouse_over,ppt_program_action,ppt_powershell
definition_001_text = openxml office document has mouseover event leading to powershell script

[analysis_module_ssdeep]
module = saq.modules.file_analysis
class = SsdeepAnalyzer
enabled = yes
; the location of the ssdeep hashes to match against
; generated using ssdeep files_to_hash > etc/ssdeep_hashes
; NOTE - make sure this file doesn't use the characters :
ssdeep_hashes = etc/ssdeep_hashes
; maximum file size (in bytes)
maximum_size = 2048000
; minimum matching threashold (in percent) before the file is tagged as ssdeep
ssdeep_match_threshold = 90

[analysis_module_archive]
module = saq.modules.file_analysis
class = ArchiveAnalyzer
enabled = yes
; comma separated list of excluded mime types we do not want to extract
excluded_mime_types=application/x-shockwave-flash,application/vnd.tcpdump.pcap
; if an archive has more than max_file_count files then we do not analyze it
; more attacks only have a single file inside the zip
; we want to avoid analyzing archives with thousands of files
max_file_count = 6
; use a different max file count for jar files
max_jar_file_count = 80
; the maximum amount of time (in seconds) to wait for 7z to complete
; 7z can go nuts and consume all system memory, so this tries to prevent that
timeout = 5

[analysis_module_olevba_v1_2]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_2
enabled = yes
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 1
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 30

[analysis_module_officeparser_v1_0]
module = saq.modules.file_analysis
class = OfficeParserAnalyzer_v1_0
enabled = yes
; path to the olevba_wrapper script (required)
officeparser_path = /usr/local/bin/officeparser
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with officeparser_failed
timeout = 90

[analysis_module_mhtml]
module = saq.modules.file_analysis
class = MHTMLAnalysisModule
enabled = yes

[analysis_module_pcodedmp]
module = saq.modules.file_analysis
class = PCodeAnalyzer
enabled = yes
; full path to the pcodedmp command line utility
pcodedmp_path = /usr/local/bin/pcodedmp

[analysis_module_office_xml_rel]
module = saq.modules.file_analysis
class = OfficeXMLRelationshipExternalURLAnalyzer
enabled = yes

[analysis_module_msoffice_encryption_analyzer]
module = saq.modules.email
class = MSOfficeEncryptionAnalyzer
enabled = yes

; for scanning the email for wordlists
; what is the minimum
range_low = 4
; and maximum size of the passwords to try
range_high = 10
; how many bytes into the email do you want to look?
byte_limit = 1024
; and what is the maximum size of the word list created?
list_limit = 1000

[analysis_module_xml_binary_analyzer]
module = saq.modules.file_analysis
class = XMLBinaryDataAnalyzer
enabled = yes

[analysis_module_xml_plain_text_analyzer]
module = saq.modules.file_analysis
class = XMLPlainTextAnalyzer
enabled = yes

[analysis_module_extracted_ole_analyzer]
module = saq.modules.file_analysis
class = ExtractedOLEAnalyzer
enabled = yes
; comma separated list of things to search for in the output of the file command
suspect_file_type = ms windows shortcut
; comma separated list of extracted file extensions that would be considered suspect here
suspect_file_ext = vbs,jse,exe,jar,lnk,ps1,bat,scr,hta,wsf,cmd,vbe,wsc

[analysis_module_office_file_archiver]
module = saq.modules.file_analysis
class = OfficeFileArchiver
enabled = yes

; the directory (relative to SAQ_DATA) to store the archived office documents
office_archive_dir = archive/office

[analysis_module_file_type]
module = saq.modules.file_analysis
class = FileTypeAnalyzer
enabled = yes

[analysis_module_pdf_analyzer]
module = saq.modules.file_analysis
class = PDFAnalyzer
enabled = yes
; path to pdfparser
pdfparser_path = bin/pdf-parser.py

[analysis_module_pdftotext]
module = saq.modules.file_analysis
class = PDFTextAnalyzer
enabled = yes
timeout = 5
; path to pdftotext
pdftotext_path = /usr/bin/pdftotext

[analysis_module_yara_scanner_v3_4]
module = saq.modules.file_analysis
class = YaraScanner_v3_4
enabled = yes
; NOTE yara configuration is in the global [yara] section
context_bytes = 64
; amount of time (in minutes) a local scanner stays available
local_scanner_lifetime = 5

[analysis_module_binary_file_analyzer]
module = saq.modules.file_analysis
class = BinaryFileAnalyzer
enabled = yes

[analysis_module_microsoft_script_encoding_analysis]
module = saq.modules.file_analysis
class = MicrosoftScriptEncodingAnalyzer
enabled = yes

; the full path to the program used to decrypt these stupid things
decryption_program = bin/jscript.decode.pl

[analysis_module_wildfire]
module = saq.modules.wildfire
class = WildfireAnalyzer
enabled = no
frequency = 10
api_key = 2e2c9a2b743ece062e96aa3794ab52c5 
use_proxy = yes

; the total amount of time (in minutes) before we time out a wildfire submission
timeout = 10

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

[analysis_module_cuckoo]
module = saq.modules.cuckoo
class = CuckooAnalyzer
; currently disabled since we have issues with production cuckoo instance (8/25/2016)
enabled = no
use_proxy = no
frequency = 10
timeout = 10
protocol = http
hosts = 

; cuckoo scores files between 0 and 10.0
; scores that exceed this threshold are tagged as malicious
threat_score_threshold = 9.0

# map file extensions to sandbox configurations (work with intel team on this)
#pdf = config1
#jar = config1
#doc = config1
#docx = config1
#docm = config1
#xls = config1
#xlsx = config1
#ppt = config1
#pptx = config1
#rtf = config1
# special default if nothing else matches
default = config1

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

[analysis_module_vxstream_file_analyzer]
module = saq.modules.vx
class = VxStreamFileAnalyzer
enabled = no

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

[analysis_module_vxstream_hash_analyzer]
module = saq.modules.vx
class = VxStreamHashAnalyzer
enabled = no

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

[analysis_module_falcon_file_analyzer]
module = saq.modules.vx
class = FalconFileAnalyzer
enabled = no

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

; This module works for md5, sha1, sha256
[analysis_module_falcon_hash_analyzer]
module = saq.modules.vx
class = FalconHashAnalyzer
enabled = no

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

[analysis_module_url_extraction]
module = saq.modules.file_analysis
class = URLExtractionAnalyzer
enabled = yes
; maximum file size in megabytes
max_file_size = 10

[analysis_module_cloudphish]
module = saq.modules.cloudphish
class = CloudphishAnalyzer
enabled = yes

; how long to wait for cloudphish to respond to a request (in seconds)
timeout = 5
; should we use a proxy to access cloudphish?
use_proxy = no
; how often we check on the status of a submission (in seconds)
frequency = 5
; how long to wait in total for a URL to be analyed by cloudphish (in seconds)
query_timeout = 300
; how many cloudphish requests are allowed for a single analysis (requests that generate work for ACE)
cloudphish_request_limit = 5

;cloudphish.1 = cloudphish1.local:443

[analysis_module_cloudphish_request_analyzer]
module = saq.modules.cloudphish
class = CloudphishRequestAnalyzer
enabled = yes

[analysis_module_crawlphish]
module = saq.modules.url
class = CrawlphishAnalyzer
enabled = yes

; path to whitelisted netloc
whitelist_path = etc/crawlphish.whitelist
; path to whitelisted path regexes
regex_path = etc/crawlphish.path_regex
; path to blacklisted netloc
blacklist_path = etc/crawlphish.blacklist
; when we are deciding what to crawl we consider if a given hostname (or ip address) is an "uncommon network"
; which means nobody hardly ever goes there
; this value determines the threshold for how many total connections (ever) is considered "yeah we go there"
uncommon_network_threshold = 70
; the user-agent to send with requests
user-agent = Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36
; how long to wait (in full seconds) until a request is timed out
timeout =  8
; maximum file size (in MB)
max_download_size = 10
; maximum file name length of downloaded files (in bytes) file names are truncated past this value
max_file_name_length = 50
; how often we reload from crits (in seconds)
crits_refresh_frequency = 300
; how long do wait until we start trying to query brocess again? (in seconds)
cooldown_period = 60
; in some cases you will want to update the brocess database with requests made with crawlphish
; this is true in cases where you do not have http visibility
update_brocess = yes
; comma separate list of proxies to use
; if a request results in a 4** or 5** level error, or errors out, then the next proxy in the list will be attempted
; the special value GLOBAL refers to the global proxy settings (see [proxy])
proxies = GLOBAL

[analysis_module_live_browser_analyzer]
module = saq.modules.url
class = LiveBrowserAnalyzer
enabled = yes

; path to the script that downloads the screenshots from the render server
; TODO get rid of this full path
get_screenshot_path = bin/get_screenshot
; remote server to be used
remote_server = rendersandbox1.local
; timeout for requests (in seconds)
timeout = 30

[analysis_module_protected_url_analyzer]
module = saq.modules.url
class = ProtectedURLAnalyzer
enabled = yes

[analysis_module_collect_file]
module = saq.modules.collect_file
class = CollectFileAnalyzer
enabled = yes
; how log to delay before trying to collect the file again (in seconds)
delay = 300
; how many hours to keep trying before finally giving up
timeout_hours = 24

[analysis_module_rtfole]
module = saq.modules.file_analysis
class = RTFOLEObjectAnalyzer
enabled = yes
; full path to the rtfobj.py script from oletools package
rtfobj_path = /usr/local/bin/rtfobj
; comma separated list of extensions that are automatically suspect if found inside an RTF OLE object
suspect_ext = js,jse,vbs,vbe,ps1,exe,jar,zip
suspect_mime_type = application/x-dosexec

[analysis_module_extracted_rtf_analyzer]
module = saq.modules.file_analysis
class = ExtractedRTFAnalyzer
enabled = yes
; comma separated list of extensions, mime types and file types that are automatically suspect if found inside an RTF
suspect_ext = js,jse,vbs,vbe,ps1,exe,jar,zip,lnk
suspect_mime_type = application/x-dosexec
suspect_file_type = PE32 executable,MS Windows shortcut

[analysis_module_advanced_link_analyzer]
module = saq.modules.advanced
class = AdvancedLinkAnalyzer
enabled = yes

[analysis_module_message_id_analyzer]
module = saq.modules.email
class = MessageIDAnalyzer
enabled = yes

; how long to wait for a message_id to become available
; this only matters for message_ids with the "delay" directive
; format is HH:MM:SS (default 1 hour)
timeout = 00:10:00

[analysis_module_vbscript_analyzer]
module = saq.modules.file_analysis
class = VBScriptAnalyzer
enabled = yes

; how many characters are considered to be a large hex string
large_hex_string_size = 50

; how many hex strings of size greater than this is considered suspect
large_hex_string_quantity = 50
large_hex_string_quantity_count = 100

; what percentage of the file that is hex string is considered suspect (between 0.0 and 0.99)
hex_string_percentage_limit = 0.15

[analysis_module_nowhitespace_analyzer]
module = saq.modules.file_analysis
class = NoWhiteSpaceAnalyzer
enabled = yes

[analysis_module_meta_refresh_extraction]
; NOTE this is only configured for the cloudphish engine
module = saq.modules.file_analysis
class = MetaRefreshExtractionAnalyzer
enabled = yes

[analysis_module_carbon_black_asset_ident]
module = saq.modules.asset
class = CarbonBlackAssetIdentAnalyzer
enabled = yes

relative_duration_before = 08:00:00
relative_duration_after = 00:15:00

; the maximum number of hostname observables to add
; this is to prevent the case when, for example, 192.168.1.1 is found and you happen to have
; hundreds of devices on some kind of wireless device at home
; if there are more than hostname_limit then we don't add any because you can't really tell which one it was
hostname_limit = 2

;
; DEPRECATED ANALYSIS MODULES
;

# DEPRECATED
[analysis_module_brotex_http_package_analyzer]
module = saq.modules.http
class = BrotexHTTPPackageAnalyzer
enabled = no
; relative path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist
; the maximum amount of requests we'll scan in a single HTTP request
; typically the things we're looking for is a single request, maybe a dozen or so if it's a webpage with images
; set this to 0 to disable (scan everything)
maximum_http_requests = 30

[analysis_module_o365_block_analyzer]
module = saq.modules.email
class = Office365BlockAnalyzer
enabled = no
valid_from_addresses = 

[analysis_module_o365_auto_disposition]
module = saq.modules.email
class = Office365AutoDispositionModule
enabled = no
; the user_id used to disposition the alert
user_id = 
; the minimum number of detections an alert needs to be auto disp
min_detections = 2
; how long to wait for the alerts to show up (in minutes)
timeout = 60
; how often to check for the alerts (in seconds)
frequency = 60

[analysis_module_brotex_smtp_package_analyzer]
module = saq.modules.email
class = BrotexSMTPPackageAnalyzer
enabled = no

[analysis_module_brotex_smtp_stream_archiver]
module = saq.modules.email
class = BrotexSMTPStreamArchiveAction
enabled = no
; the directory to store archives emails (relative to SAQ_HOME)
archive_dir = archive/smtp_stream

[analysis_module_pcap_extraction_service]
module = saq.modules.pcap
class = PcapExtractionServiceAnalyzer
enabled = no
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; relative time capture (see -d option of the extract script)
relative_duration = 00:02:00
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 6
; network location of pcap extraction service
host = 
port = 
; how often do we check to see if the job has completed (in seconds)
check_interval = 5
; how many errors can happen before we stop trying?
error_count_limit = 3

[analysis_module_tshark]
module = saq.modules.pcap
class = TsharkPcapAnalyzer
enabled = no

[analysis_module_bro]
module = saq.modules.pcap
class = BroAnalyzer
enabled = no
; full path to bro binary
bro_bin_path = /nsm/bro/bin/bro
; full path to the ace bro file to run for file extraction
; TODO

[analysis_module_ipdb]
module = saq.modules.ipdb
class = IPDBAnalyzer
enabled = no
csv_file = etc/ip_database.csv
csv_file_encoding = latin-1

[analysis_module_asn]
module = saq.modules.asn
class = ASNAnalyzer
enabled = no
exclude_internal_network = observable_group:internal

netmask_to_asn_file = etc/asn/data-raw-table
netmask_to_asn_file_encoding = latin-1
asn_to_owner_file = etc/asn/data-used-autnums
asn_to_owner_file_encoding = latin-1

[analysis_module_email_history_analyzer_v1]
module = saq.modules.email
class = EmailHistoryAnalyzer_v1
enabled = no
semaphore = splunk
; we go back a bit to try to catch phish sitting idle in the inbox
relative_duration_before = 72:00:00
relative_duration_after = 02:00:00

[analysis_module_mwzoo]
module = saq.modules.file_analysis
class = MalwareZooAnalyzer
enabled = no
mwzoo_home = /opt/mwzoo

[analysis_module_olevba_v1_0]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_0
enabled = no
; path to the olevba_wrapper script (required)
olevba_wrapper_path = bin/olevba_wrapper.py
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 2
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 90

[analysis_module_olevba_v1_1]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_1
enabled = no
; path to the olevba_wrapper script (required)
olevba_wrapper_path = bin/olevba_wrapper.py
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 2
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 30

; DEPRECATED (see analysis_module_office_file_archiver instead)
[analysis_module_ole_archiver_v1_0]
module = saq.modules.file_analysis
class = OLEArchiver_v1_0
enabled = no
; directory to contain all the archived office documents
ole_archive_dir = archive/ole

[analysis_module_vxstream_v1_0]
module = saq.modules.vx
class = VxStreamAnalyzer_v1_0
enabled = no
expect_proxy = 

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = yes

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

[analysis_module_smtp_stream_analyzer]
module = saq.modules.email
class = SMTPStreamAnalyzer
enabled = no

; how many lines do we read in a file to figure out if it's an SMTP protocol session
protocol_scan_line_count = 30

;
; PROFILE POINT ANALYSIS
;

[profile_points]
; a comma separated list of modules that contain profile point classes to load
modules = saq.modules.profilepoint
; the minimum threshold (in percent range 0 to 100) that determines if a profile point is displayed as "matched"
display_threshold = 35
; a comma separated list of profile point classes to disable
disabled_profile_points = 

;
; SITE MODULES
;

;
; TAGGING MODULES
;

; tag people in specific groups in the organziation
[analysis_module_user_tagger]
module = saq.modules.user
class = UserTaggingAnalyzer
enabled = yes

; location of the cache (use saq update-organization to build this file)
json_path = etc/organization.json

; FORMAT: group_GROUPNAME = BOSS,LEVELS_DEEP,TAG
; GROUPNAME = some unique name to give the group
; BOSS = the person at the organzation leading the group
; LEVELS_DEEP = how far deep in direct reports to include people in the group
; TAG = what tag to give people in this group

; CEO and direct reports
; group_ceo - userid,2,executive

[analysis_module_site_tagger]
module = saq.modules.tag
class = SiteTagAnalyzer
enabled = yes
;
; the CSV file that contains the tagging definitions
; format is as follows
; o_types,match_type,ignore_case,value,tags
; o_types : list of valid observable types separated by | (pipe) character
; match_type: one of the following values
;
; TYPE          DESCRIPTION
; default       default string matching (exact match)
; glob          unix-style glob pattern matching
; regex         regular expression
; cidr          CIDR network matching
; subdomain     subdomain matching
;
; ignore_case : boolean (see https://docs.python.org/3.4/library/stdtypes.html#truth) to ignore case or not
; value : the value to match against
; tags : list of tags separated by | (pipe) character to apply if observable matches
;
csv_file = etc/site_tags.csv

[disabled_modules]
; zero or more analysis module section names that should be disabled
; this is for configurations that require that certain modules not run
; for example: an environment where samples should not be submitted to sandboxes

analysis_module_wildfire = yes
analysis_module_ole_archiver_v1_0 = yes
analysis_module_olevba_v1_0 = yes
analysis_module_olevba_v1_1 = yes

;
; TAG PRIORITIES AND SETTINGS
;

[tags]

; valid values are
; hidden
; special (white)
; fp (green)
; info  (gray)
; warning (yellow)
; alert (red)
; critical (red)

; defaults to info if the tag isn't in this list
; warning and above trigger alerts

mail_to = hidden
delivered_to = hidden
mail_from = hidden
reply_to = hidden
no_render = hidden

high_fp_frequency = special
whitelisted = special
encrypted_email = special
decrypted_email = special
tracked = special

debug = fp
qualys = fp
infosec = fp
p2p = fp
frequent_conversation = fp
guest_wireless = fp

new_sender = interesting

treasury = warning
suspicious_url  = warning
suspect = warning
olevba = warning
;olevba_failed = warning
officeparser_failed = warning
rtf_binary = warning
unexpected_binary_data = warning
microsoft_script_encoding = warning

office365_block = alert
pos = alert
vioc = alert
phish = alert
executive = alert
apt = alert
av = alert
dc = alert
malicious_url = alert
malicious = alert
ssdeep = alert
rat = alert
script_in_zip = alert
exe_in_zip = alert
lnk_in_zip = alert
suspect_ole_attachment = alert
;high_mal_frequency = alert

critical = critical

[tag_css_class]

; css classes to use for the various levels
hidden =
special = label-warning label-special
fp = label-success
info = label-default
warning = label-warning
interesting = label-warning label-interesting
alert = label-danger
critical = label-danger

;
; MODULE GROUPS
;

[module_group_correlation]
; all the modules you'd want minus any special modules designed for specific systems
analysis_module_active_directory_analyzer = yes
analysis_module_advanced_link_analyzer = yes
analysis_module_alert_correlation = yes
analysis_module_bluecoat_analysis_by_dst_v1 = yes
analysis_module_bluecoat_analysis_by_src_v1 = yes
analysis_module_carbon_black_asset_ident = yes
analysis_module_carbon_black_netconn_source_analysis = yes
analysis_module_carbon_black_process_analysis_v1 = yes
analysis_module_cloudphish = yes
analysis_module_cloudphish_url_email_pivot_analyzer = yes
analysis_module_collect_file = yes
analysis_module_crits_observable_analyzer = yes
analysis_module_dlp_process = yes
analysis_module_dlp_process_hash_analysis_v1 = yes
analysis_module_dns_request_analysis_v1 = yes
analysis_module_elk_snort = yes
analysis_module_email_address_analyzer = yes
analysis_module_email_history_analyzer_v1 = yes
analysis_module_email_history_analyzer_v2 = yes
analysis_module_exploit_kit_proxy_analyzer = yes
analysis_module_live_browser_analyzer = yes
analysis_module_netbios_analyzer = yes
analysis_module_network_identifier = yes
analysis_module_pan_snort_correlation = yes
analysis_module_pan_threats = yes
analysis_module_pcap_conversation_extraction = yes
analysis_module_process_guid_analysis = yes
analysis_module_snort = yes
analysis_module_snort_signature_analysis_v1 = yes
analysis_module_symantec = yes
analysis_module_url_email_pivot_analyzer = yes
analysis_module_user_analyzer = yes
analysis_module_vpn_analyzer = yes
analysis_module_vt_hash_analyzer = yes
analysis_module_vt_hash_downloader = yes
analysis_module_falcon_hash_analyzer = yes

[module_group_file]
; everything you would need for file analysis
analysis_module_archive = yes
analysis_module_binary_file_analyzer = yes
analysis_module_collect_file = yes
analysis_module_extracted_ole_analyzer = yes
analysis_module_extracted_rtf_analyzer = yes
analysis_module_file_hash_analyzer = yes
analysis_module_file_path_analysis = yes
analysis_module_file_type = yes
analysis_module_mhtml = yes
analysis_module_microsoft_script_encoding_analysis = yes
analysis_module_nowhitespace_analyzer = yes
analysis_module_office_file_archiver = yes
analysis_module_officeparser_v1_0 = yes
analysis_module_ole_archiver_v1_0 = yes
analysis_module_olevba_v1_2 = yes
analysis_module_pcodedmp = yes
analysis_module_pdf_analyzer = yes
analysis_module_pdftotext = yes
analysis_module_rtfole = yes
analysis_module_site_tagger = yes
analysis_module_ssdeep = yes
analysis_module_url_extraction = yes
analysis_module_vbscript_analyzer = yes
analysis_module_vxstream_file_analyzer = yes
analysis_module_vxstream_hash_analyzer = yes
analysis_module_wildfire = yes
analysis_module_xml_binary_analyzer = yes
analysis_module_xml_plain_text_analyzer = yes
analysis_module_yara_scanner_v3_4 = yes
analysis_module_falcon_file_analyzer = yes
analysis_module_falcon_hash_analyzer = yes

[module_group_email]
; everything related to email scanning
analysis_module_email_analyzer = yes
analysis_module_bro_smtp_analyzer = yes
analysis_module_email_conversation_attachment_analyzer = yes
analysis_module_email_conversation_frequency_analyzer = yes
analysis_module_email_link_analyzer = yes
analysis_module_encrypted_archive_analyzer = yes
analysis_module_mailbox_email_analyzer = yes
analysis_module_message_id_analyzer = yes
analysis_module_msoffice_encryption_analyzer = yes
;analysis_module_o365_block_analyzer = yes
analysis_module_smtp_stream_analyzer = yes

[module_group_common]
analysis_module_cloudphish = yes
analysis_module_correlated_tag_analyzer = yes
analysis_module_crits_analyzer = yes
;analysis_module_detection = yes
analysis_module_site_tagger = yes
analysis_module_user_tagger = yes
analysis_module_hal9000 = yes

;
; ENGINE CONFIGURATION
;

[engine]
; analysis pool settings
; after the pool size has been decided, you can assign individual processes to prioritize certain 
; types of work by assigning an "analysis_mode" to a number of workers
; these workers will first look to see if any work is available for a given analysis mode
; if none is available then it will just pull the next work item from the FIFO queue
; NOTE if analysis_pool_size is less that the total number of assignments, then analysis_pool_size
; will be reconfigured to meet this new requirement and a warning will be logged
;
; the format of this configuration is analysis_pool_size_MODE = int
; where MODE is one of the modes listed under ANALYSIS MODES
;
; analysis_pool_size_correlation = 4
; analysis_pool_size_email = 8
; analysis_pool_size_http = 2
;
; if NO analysis pools are specified then a single pool with no equal prioirty will be created with
; a size equal to the number of CPU cores

; how often to discard the worker processes and create new ones (in seconds) 
auto_refresh_frequency = 1800

; the default analysis mode if none is specified, or an unknown analysis mode is specified
default_analysis_mode = analysis

; a comma separated list of analysis modes this engine will handle
; leaving this empty will default to supporting all analysis modes
local_analysis_modes = 

; the nodes database table keeps track of all the ace nodes that are currently available
; this settings specifies how often (in seconds) we update the table with our current information
node_status_update_frequency = 30

; if this is set to yes then any analysis that fails is copied to a directory for review later
; NOTE this can take a lot of disk space
copy_analysis_on_error = no

; in some cases you might want your work to be performed on a different hard drive
; if this value is set then new non-alert analysis will be performed in the given directory
; relative to SAQ_HOME
work_dir = 

; when an analyst dispositions an alert ace will stop analyzing it if the alert is in correlation mode
; this specifies how often ace checks the database for the disposition value (in seconds)
alert_disposition_check_frequency = 5

; a comma separated list of analysis modes that will NOT become alerts if detections are made
; any analysis in correlation mode is already an alert
; any analysis in dispositioned mode was already an alert that was dispositioned
; if you add more custom analysis modes that should NOT become alerts (such as multiple stages of analysis modes)
; then you need to add them to this list
non_detectable_modes = correlation, dispositioned

; ----------------------------------------------------------------------------

[cloudphish]
; the location of cached data downloaded by the cloudphish engine (relative to DATA_DIR)
cache_dir = cloudphish

;
; ANALYSIS MODES
;

; configures what analysis modules should run for the various analysis modes
;

; format is as follows
; [analysis_mode_MODE]
; module_groups = group1, groups2, ...
; cleanup = yes|no
; analysis_module_blah = yes|no

[analysis_mode_http]
; mode used for HTTP stream analysis
module_groups = common, file
cleanup = yes
analysis_module_bro_http_analyzer = yes

maximum_cumulative_analysis_warning_time = 30
maximum_cumulative_analysis_fail_time = 120
maximum_analysis_time = 20

[analysis_mode_email]
; mode used for email (rfc822 and SMTP stream) analysis
module_groups = common, file, email
cleanup = yes
analysis_module_email_archiver = yes
analysis_module_email_logger = yes

maximum_cumulative_analysis_warning_time = 30
maximum_cumulative_analysis_fail_time = 120
maximum_analysis_time = 20

[analysis_mode_binary]
; default mode for binary (executable) file analysis
module_groups = common, file
cleanup = yes

maximum_cumulative_analysis_warning_time = 30
maximum_cumulative_analysis_fail_time = 120
maximum_analysis_time = 20

[analysis_mode_file]
; mode used for generic file analysis
module_groups = common, file
cleanup = yes

maximum_cumulative_analysis_warning_time = 30
maximum_cumulative_analysis_fail_time = 120
maximum_analysis_time = 20

[analysis_mode_correlation]
; mode used for analyzing alerts
module_groups = common, file, email, correlation
cleanup = no

; NOTE once an analysis is set to correlation is automatically becomes an alert
;analysis_module_detection = no

[analysis_mode_analysis]
; default mode if none other is specified
cleanup = yes
module_groups = common, file, email, correlation

[analysis_mode_cloudphish]
; urls submitted to cloudphish (via the cloudphish API calls) are analyzed in this mode
module_groups = common, file
cleanup = yes
analysis_module_cloudphish = no
analysis_module_cloudphish_request_analyzer = yes
analysis_module_crawlphish = yes
analysis_module_protected_url_analyzer = yes
analysis_module_user_tagger = no

maximum_cumulative_analysis_warning_time = 30
maximum_cumulative_analysis_fail_time = 120
maximum_analysis_time = 20

[analysis_mode_cli]
; more of a debug mode, used by the command line correlate subcommand of the ace executable script
module_groups = common, file, email, correlation
cleanup = no
;analysis_module_alert_disposition_analyzer = no
analysis_module_hal9000 = no
analysis_module_faqueue_alert_analyzer = no

[analysis_mode_dispositioned]
; when an alert is dispositioned by an analyst, any current analysis stops
; and the mode is changed to dispositioned and re-analyzed
; this gives certain analysis modules a change to do something with the value of the disposition
; this more of a limited list of analysis modules since few of them act this way
module_groups = 
cleanup = no
analysis_module_hal9000 = yes
analysis_module_faqueue_alert_analyzer = yes
;analysis_module_detection = no
