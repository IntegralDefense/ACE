[global]

;           _____ ______ 
;     /\   / ____|  ____|
;    /  \ | |    | |__   
;   / /\ \| |    |  __|  
;  / ____ \ |____| |____ 
; /_/    \_\_____|______|
;                        
; Alert Correlation Engine

;
; DEFAULT CONFIGURATION
; *** DO NOT EDIT THIS FILE UNLESS YOU ARE A DEVELOPER ***
; override this configuration by creating the file etc/saq.ini with the local configuration settings 
;

; the default company name and id to use for alerts generated by engines using this configuration file
; SELECT * FROM company
company_name = OVERRIDE
company_id = OVERRIDE
; shows up in the GUI so you know which instance you're logged into
instance_name = OVERRIDE
; the name of this node, usually this is the fully qualified domain name
node = OVERRIDE
; this can be either PRODUCTION, QA or DEV
; used by some routines to prevent a user from making a horrible mistake
; if this setting is missing then the default value taken is PRODUCTION
instance_type = OVERRIDE
; forces the entire application to run under a single thread
debug = no
; global data directory that has all the things
data_dir = data
; temp directory for various whatever
tmp_dir = var/tmp
; directory that contains any stack traces for review
error_reporting_dir = error_reports
; list of email addresses (comma separated) that get these reports sent to them automatically
; you can leave this empty
error_reporting_email = OVERRIDE

; the amount of time (in minutes) in between reporting of the same condition
; set to 0 to disable condition reporting
condition_reporting_delay = 480

; comma separated list of local domains
local_domains = OVERRIDE

; set to yes to log all SQL commands executed by the server
log_sql = no

; set to yes to log all SQL commands and their execution time
log_sql_exec_times = no

; set this to True to enable semaphores
; you'll definitely want this to be True in production settings
enable_semaphores = yes

; set this to the total number of analysis engines (of any type) you have running
global_engine_instance_count = 1

; the number of days alerts set to IGNORE will last until they are deleted from the system
ignore_days = 1

; the number of days alerts set to FALSE_POSITIVE will last until they are reset
fp_days = 30

; when alerts are being processed they are locked for processing
; if a process dies during process then a stale lock could linger
; the amount of time a lock is considered valid (in MM:SS format)
lock_timeout = 240:00

; amount of time (in seconds) between chronos global lock keep alive messages
lock_keepalive_frequency = 10

; amount of time (in minutes) that we expect analysis to take, in general
; we use this to warn ourselves that something might be wrong with logic in a module
maximum_cumulative_analysis_warning_time = 5

; amount of time (in minutes) to give analysis (in total) before we bail entirely
maximum_cumulative_analysis_fail_time = 15

; amount of time (in seconds) that we expect a single analysis module to take
maximum_analysis_time = 60

[SSL]
ca_chain_path = ssl/ca-chain.cert.pem

[proxy]
; leave these values blank if you do not need to use a proxy
host = OVERRIDE
port = OVERRIDE
user = OVERRIDE
password = OVERRIDE


; additional proxy settings can be added and referred to by analysis modules that might use more than one
; for example, the crawlphish analysis module can use multiple proxies
; additional proxies have the format [proxy_name] where name is a unique name for the proxy

;[proxy_tor]
; tor socks proxy configuration
; leave these blank if you're not using tor
;host = 
;port =
;user =
;password =


[gui]
; this is the single threaded developer server GUI config
; production used apache configuration (see etc/saq_apache.conf)
; binding address to incoming requests
listen_address = 0.0.0.0
listen_port = 5000
; ssl parameter (required)
ssl_cert = OVERRIDE
ssl_key = OVERRIDE

; the base address for the GUI (used to build ACE permalinks)
base_uri = OVERRIDE

; default company for manual analysis (the default option from the drop down)
; 3 == integral
default_company_id = OVERRIDE

; define what companies are "core" companies
; these are the company IDs separated by commas
core_companies = OVERRIDE

; secret key used by flask
secret_key = OVERRIDE

;
; define the global SLA settings
; see below to configure SLA for a specific company or alert type
;

[SLA]
; set this to no to disable SLA globally (useful for debug/qa systems)
enabled = no
; the following values are the global defaults
; the amount of time (in hours) to SLA
time_to_dispo = 8
; the amount of time (in hours) before SLA is exceeded to warn
approaching_warn = 1
; comma separated list of alert types that are excluded from SLA
excluded_alert_types = 

; additional settings can be defined for specific alerts
; the format is as follows
; [SLA_unique_name] where _unique_name can be any unique descriptive name
; property = the property of the alert to match
; value = the value of the property to match
; these settings can be applied
; enabled =
; time_to_dispo =
; approaching_warn =

[client]
; the URI the client should connect to to submit alerts (see api_port below)
uri = OVERRIDE
key = blah

[server]
; binding address to incoming requests
listen_address = 0.0.0.0
listen_port = 5002
; local storage for files as they come into the system
incoming_dir = var/incoming

[network_client_ace]
; contains configuration data for sending alerts to ace
; this should essentially match what is configured for engine_ace
remote_host = OVERRIDE
remote_port = 12343

ssl_hostname = OVERRIDE
ssl_cert = OVERRIDE
ssl_key = OVERRIDE
ca_path = ssl/ca-chain.cert.pem

;ssl_hostname = nakylexsec101
;; this should match the ssl_root_cert_path in the engine config
;ssl_root_cert_path = ssl/ashland_cybersecurity.pem

; path to a subdirectory (relative to SAQ_HOME) where failed alert submissions can be stored
failed_dir = failed_submissions

[anp_defaults]
; default settings for ACE Network Protocol client/server

; a comma separated list of host:port pairs of anp remote nodes for this engine type
anp_nodes = 127.0.0.1:41433
; the listening address and port for this anp server node
; only valid if mode = server
anp_listening_address = 127.0.0.1
anp_listening_port = 41433

; amount of time (in seconds) that an anp node is ignored becuase of failed connection attempts
; or BUSY command results
anp_retry_timeout = 5

ssl_ca_path = ssl/ca-chain.cert.pem
ssl_cert_path = ssl/web/localhost.cert.pem
ssl_key_path = ssl/web/localhost.key.pem
ssl_hostname = localhost

[network_semaphore]
; the address of the network semaphore server (used to bind and listen)
bind_address = OVERRIDE
bind_port = 53559

; the address of the network semaphore server to the clients that want to use them
; could be the same as the bind_adress and bind_port above
remote_address = OVERRIDE
remote_port = 53559

; comma separated list of source IP addresses that are allowed to connect
allowed_ipv4 = 127.0.0.1
; directory that contains metrics and current status of semaphores
stats_dir = var/network_semaphore

; SEMAPHORE CAPACITY LIMITS
;
; each of these represents a resource and the number of simultaneous connections allowed to it
semaphore_pcap = 1
semaphore_splunk = 1
semaphore_carbon_black = 1

[network_configuration]
; this section defines what the managed network looks like
;
; command separated list of CIDR notation for managed networks
managed_networks = 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12

[mediawiki]
domain = https://wiki.local/
; main url prefix for mediawiki
uri = https://wiki.local/display/integral/AlertContext
; url suffix for alert documentation
alert_suffix = #AlertContext-

[smtp]
; smtp information for sending emails
server = OVERRIDE
mail_from = OVERRIDE

; deprecated config setting
remediation_address = 

[remediation]
; EWS remediation host and port
ews_host = OVERRIDE
ews_port = 3100

; comma separated list of email addresses to ignore when performing remediation
; these are typically functional email addresses, such as those used to journal
excluded_emails=

[email_archive]
; if this system is archving emails, this determines what section to use for the database config
; NOTE this is the name of the section without the leading database_ (legacy issue)
primary = email_archive

[memcached]
; the address of the memcached system used by ACE
client_address = unix:var/memcached.socket

#
# unix_socket is typically /var/run/mysqld/mysqld.sock
#

[database_ace]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE
;ssl_key = ssl/mysql/client-key.pem
;ssl_cert = ssl/mysql/client-cert.pem
;ssl_ca = ssl/mysql/ca-cert.pem

[database_brocess]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE
; how long do we wait for brocess queries to complete (in seconds)
; these queries should complete super fast
; failure to complete the timeout will send the analysis module using brocess into cooldown mode
query_timeout = 5

[database_hal9000]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_workload]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_email_archive]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_cloudphish]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[database_vt_hash_cache]
hostname = OVERRIDE
unix_socket = OVERRIDE
database = OVERRIDE
username = OVERRIDE
password = OVERRIDE

[ldap]
enabled = yes
tivoli_enabled = no
; the LDAP server (probably your domain controller)
ldap_server = OVERRIDE
tivoli_server = 
; the port the LDAP server listens to (defaults to 389)
ldap_port = 389
tivoli_ldap_port = 389
; user account to use for authentication
ldap_bind_user = OVERRIDE
tivoli_bind_user = 
; enter the LDAP password, or PROMPT to force the system to prompt
;ldap_bind_password = PROMPT
ldap_bind_password = OVERRIDE
tivoli_bind_password = 
; the base DN for searching
ldap_base_dn = OVERRIDE
tivoli_base_dn = 

[splunk]
enabled = yes
; the splunk query server
uri = OVERRIDE
; user account information for splunk
username = OVERRIDE
password = OVERRIDE
; the maximum number of results a single query can generate
max_result_count = 100
; the relative duration of time to search for around the event
relative_duration_before = 00:15:00
relative_duration_after = 00:00:01

[elk]
enabled = no
; the URI used for queries (format hostname[:port]/path)
uri =
; the default cluster to search
; leave this value empty to not specify the cluster
; use * to search all clusters
cluster = 
; user account information for elk (with search guard)
; leave this blank if you're not using search guard
username =
password = 

; the maximum number of results a single query can generate
max_result_count = 100
; the default relative duration of time to search if not specified
relative_duration_before = 00:15:00
relative_duration_after = 00:00:01

[splunk_logging]
splunk_log_dir = splunk_logs

[elk_logging]
; the base directory for all log files destined for elasticsearch
elk_log_dir = logs/export

[crits]
url = OVERRIDE
mongodb_uri = OVERRIDE
api_key = OVERRIDE
activity_url = OVERRIDE
indicators_endpoint = /api/v1/indicators/
username = 
; path to sqlite database cache of crits indicators (relative to SAQ_HOME)
cache_db_path = var/crits.db

[crits_indicator_type_mapping]

; indicator type mapping
; if your crits installation has different names for indicator types you can set those here
; otherwise it uses the default crits indicator names
; these were pulled from https://github.com/crits/crits/blob/master/crits/vocabulary/indicators.py on 10/16/2018
ADJUST_TOKEN = Adjust Token
API_KEY = API Key
AS_NUMBER = AS Number
AS_NAME = AS Name
BANK_ACCOUNT = Bank account
BITCOIN_ACCOUNT = Bitcoin account
CERTIFICATE_FINGERPRINT = Certificate Fingerprint
CERTIFICATE_NAME = Certificate Name
CHECKSUM_CRC16 = Checksum CRC16
CMD_LINE = Command Line
COMPANY_NAME = Company name
COOKIE_NAME = Cookie Name
COUNTRY = Country
CRX = CRX
DEBUG_PATH = Debug Path
DEBUG_STRING = Debug String
DEST_PORT = Destination Port
DEVICE_IO = Device IO
DOC_FROM_URL = Document from URL
DOMAIN = Domain
EMAIL_BOUNDARY = Email Boundary
EMAIL_ADDRESS = Email Address
EMAIL_FROM = Email Address From
EMAIL_HEADER_FIELD = Email Header Field
EMAIL_HELO = Email HELO
EMAIL_MESSAGE_ID = Email Message ID
EMAIL_ORIGINATING_IP = Email Originating IP
EMAIL_REPLY_TO = Email Reply-To
EMAIL_SENDER = Email Address Sender
EMAIL_SUBJECT = Email Subject
EMAIL_X_MAILER = Email X-Mailer
EMAIL_X_ORIGINATING_IP = Email X-Originating IP
FILE_CREATED = File Created
FILE_DELETED = File Deleted
FILE_MOVED = File Moved
FILE_NAME = File Name
FILE_OPENED = File Opened
FILE_PATH = File Path
FILE_READ = File Read
FILE_WRITTEN = File Written
GET_PARAM = GET Parameter
HEX_STRING = HEX String
HTML_ID = HTML ID
HTTP_REQUEST = HTTP Request
HTTP_RESP_CODE = HTTP Response Code
IMPHASH = IMPHASH
IPV4_ADDRESS = IPv4 Address
IPV4_SUBNET = IPv4 Subnet
IPV6_ADDRESS = IPv6 Address
IPV6_SUBNET = IPv6 Subnet
LATITUDE = Latitude
LAUNCH_AGENT = Launch Agent
LOCATION = Location
LONGITUDE = Longitude
MAC_ADDRESS = MAC Address
MALWARE_NAME = Malware Name
MD5 = MD5
MEMORY_ALLOC = Memory Alloc
MEMORY_PROTECT = Memory Protect
MEMORY_READ = Memory Read
MEMORY_WRITTEN = Memory Written
MUTANT_CREATED = Mutant Created
MUTEX = Mutex
NAME_SERVER = Name Server
OTHER_FILE_OP = Other File Operation
PASSWORD = Password
PASSWORD_SALT = Password Salt
PAYLOAD_DATA = Payload Data
PAYLOAD_TYPE = Payload Type
PIPE = Pipe
POST_DATA = POST Data
PROCESS_NAME = Process Name
PROTOCOL = Protocol
REFERER = Referer
REFERER_OF_REFERER = Referer of Referer
REGISTRAR = Registrar
REGISTRY_KEY = Registry Key
REG_KEY_CREATED = Registry Key Created
REG_KEY_DELETED = Registry Key Deleted
REG_KEY_ENUMERATED = Registry Key Enumerated
REG_KEY_MONITORED = Registry Key Monitored
REG_KEY_OPENED = Registry Key Opened
REG_KEY_VALUE_CREATED = Registry Key Value Created
REG_KEY_VALUE_DELETED = Registry Key Value Deleted
REG_KEY_VALUE_MODIFIED = Registry Key Value Modified
REG_KEY_VALUE_QUERIED = Registry Key Value Queried
SERVICE_NAME = Service Name
SHA1 = SHA1
SHA256 = SHA256
SMS_ORIGIN = SMS Origin
SOURCE_PORT = Source Port
SSDEEP = SSDEEP
TELEPHONE = Telephone
TIME_CREATED = Time Created
TIME_UPDATED = Time Updated
TRACKING_ID = Tracking ID
TS_END = TS End
TS_START = TS Start
URI = URI
URI_PATH = URI Path
USER_AGENT = User Agent
USER_ID = User ID
VICTIM_IP = Victim IP
VOLUME_QUERIED = Volume Queried
WEBSTORAGE_KEY = Webstorage Key
WEB_PAYLOAD = Web Payload
WHOIS_NAME = WHOIS Name
WHOIS_ADDR1 = WHOIS Address 1
WHOIS_ADDR2 = WHOIS Address 2
WHOIS_REGISTRANT_EMAIL_ADDRESS = WHOIS Registrant Email Address
WHOIS_TELEPHONE = WHOIS Telephone
XPI = XPI

[crits_observable_type_mappping]
; observable type mapping to crits indicator types
cidr =
ipv4 = IPV4_ADDRESS
ipv4_conversation =
fqdn = DOMAIN
hostname = DOMAIN
http_request = 
asset = IPV4_ADDRESS
user = USER_ID
url = URI
pcap = 
file = 
file_path = FILE_PATH
file_name = FILE_NAME
file_location =
email_address = EMAIL_ADDRESS
email_conversation = 
yara =
yara_rule =
indicator =
md5 = MD5
sha1 = SHA1
sha256 = SHA256
snort_sig = 
message_id = EMAIL_MESSAGE_ID
process_guid =

[yara]
; base directory where the yara scanner server (yss) runs out of (relative to SAQ_HOME)
yss_base_dir = yss
; relative directory where the unix sockets for the yara scanner server are located
yss_socket_dir = socket
; global configuration of yara rules
; format is signature_(repo|dir|file)_unique_identifier
; "repo" is a directory that is a git repository
; "dir" is a directory that contains .yar files
; "file" is a single yara file
; the value is the path to the directory or file
signature_repo_custom = etc/yara/custom
; the blacklist contains a list of rule names (one per line) to exclude from the results
blacklist_path = etc/yara.blacklist
; a directory that contains all the files that fail to scan
scan_failure_dir = scan_failures

[virus_total]
; virus total authentication
api_key = 
; lookup URL
query_url = https://www.virustotal.com/vtapi/v2/file/report
; download URL
download_url= https://www.virustotal.com/vtapi/v2/file/download
; storage directory for downloaded VT files (relative to installation dir)
cache_dir = vt_cache

[vxstream]
; the base URI for all vxstream requests (should be your local vxstream installation)
baseuri = OVERRIDE
; the environment to submit all samples into
environmentid = OVERRIDE
; authentication stuff
apikey = OVERRIDE
secret = OVERRIDE
; the baseuri to use from the GUI
gui_baseuri = OVERRIDE

[carbon_black]
; carbon black server API location and authentication
url = OVERRIDE
token = 
; site specific credential file for new CBapi 
credential_file = etc/carbon_black.auth

; the url displayed and used in the GUI for analysts
; this may be different than url
gui_url = OVERRIDE

; defines what we do with the malicious files we find
[malicious_files]
; this is where we actually store copies of the files
malicious_dir = malicious
; comma separated list of email accounts to send notifications to when new files are added
malicious_alert_recipients = 

[gpg]
; gpg is used to encrypt sensitive archived files such as emails
; the public key to use to encrypt the emails
encryption_recipient = OVERRIDE

[chronos]
; the amount of time (in seconds as a float) in between status checks for requested locks
status_check_frequency = 1.0
; location of the chronos system
host = https://localhost
port = 5032
path = chronos/
; location of the chronos system
analysis-host = analysis.local
analysis-port = 5031

; define groups of observables 
; all sections must begin with observable_group_
; and then any number of define_UNIQUE_ID = type:value
;
; these can then be referenced in module definitions by using the following specifier
; exclude_group_01 = observable_group:group_name
; where group_name is the text after observable_group_
;
; for example, say you define a group called observable_group_internal
; to exclude everything in this group from an analysis module, add the following to the module config
; exclude_blah_01 = observable_group:internal

[observable_group_internal]
; all internal IP addresses
define_internal_04 = ipv4:10.0.0.0/8
define_internal_05 = ipv4:192.168.0.0/16
define_internal_06 = ipv4:172.16.0.0/12

[observable_group_proxy]
; all known proxy nodes (bluecoat and squid)

[observable_group_smtp]
; SMTP related services

[observable_group_external_gateway]
; exit nodes for outbound traffic

[observable_group_internal_dns]
; internal BIND and AD servers for DNS

[observable_group_external_dns]
; external BIND servers for DNS

[observable_group_popular_fqdn]
define_fqdn_google = fqdn:google.com
define_fqdn_amazon = fqdn:amazon.com
define_fqdn_yahoo = fqdn:yahoo.com
define_fqdn_youtube = fqdn:youtube.com

; global observable exclusions
; things we never want to analyze because they are noisy, errors or stupid
[observable_exclusions]
; format is random_name = observable_type:observable_value

; ignore blank user IDs
exclude_user = user:-
exclude_user_unknown = user:unknown
exclude_user_system = user:system
; loopback
exclude_loopback = ipv4:127.0.0.1
; weird
exclude_invalid_ipv4 = ipv4:0.0.0.0

; office365 journaling

; super common websites
exclude_google = fqdn:google.com
exclude_youtube = fqdn:youtube.com

[deprecated_modules]
; a list of analysis modules that have been removed from the system but are still references in the data.json files
analysis_module_dlp_baseline = saq.modules.dlp:DLPBaselineAnalysis

;
; CUSTOM ALERTS
;

[custom_alerts]
debug = analysis/custom/saq_debug.html
brotex - smtp = analysis/custom/brotex_smtp.html
brotex - smtp - v2 = analysis/custom/brotex_smtp_v2.html
mailbox = analysis/custom/email_alert.html
o365 = analysis/custom/email_alert.html
brotex - http = analysis/custom/brotex_http.html
splunk - snort = analysis/custom/splunk_snort.html
splunk - av = analysis/custom/splunk_av.html
splunk - bit9 = analysis/custom/splunk_bit9.html
splunk - cb = analysis/custom/cb.html
carbonblack - watchlist = analysis/custom/cb_watchlist.html
cloudphish = analysis/custom/cloudphish.html
http = analysis/custom/http.html

;
; ANALYSIS MODULES
;

; configuration format is as follows
; [analysis_module_UNIQUE_MODULE_NAME] <-- must begin with analysis_module_ and module_name must be unique
; module = MODULE_PATH <-- the module path the AnalysisModule classes is located in
; class =  MODULE_CLASS
; enabled = yes | no
; module_groups = group1,group2,...
; exclude_UNIQUE_NAME = type:value
; expect_UNIQUE_NAME = type:value
;
; you basically want everything enabled except for the things that are broken or deprecated
; you control what runs per engine configuration
;

[analysis_module_smtp_stream_analyzer]
module = saq.modules.email
class = SMTPStreamAnalyzer
enabled = yes

; how many lines do we read in a file to figure out if it's an SMTP protocol session
protocol_scan_line_count = 30

[analysis_module_email_analyzer]
module = saq.modules.email
class = EmailAnalyzer
enabled = yes

; relative path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist

; office365 journaling will cause outbound emails to also get journaled
; set this to no to scan outbound office365 emails
scan_inbound_only = yes

[analysis_module_email_link_analyzer]
module = saq.modules.advanced
class = EmailLinkAnalyzer
enabled = yes

[analysis_module_email_archiver]
module = saq.modules.email
class = EmailArchiveAction
enabled = no

; the directory to contain the archived emails
archive_dir = archive/email
; how long to keep archived emails (in days)
expiration_days = 7

[analysis_module_encrypted_archive_analyzer]
module = saq.modules.email
class = EncryptedArchiveAnalyzer
enabled = yes

; General Analysis
[analysis_module_file_path_analysis]
module = saq.modules.file_path
class = FilePathAnalyzer
enabled = yes

[analysis_module_hal9000]
module = saq.modules.hal9000
class = HAL9000Analyzer
enabled = yes

exclude_proxy = observable_group:proxy
exclude_smtp = observable_group:smtp
exclude_external_gateway = observable_group:external_gateway
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns

; minimum sample size required for tagging
min_sample_size = 30
; percent threshold ABOVE which the observable is tagged as high_mal_frequency
mal_threshold = 90
; percent threshold BELOW which the observable is tagged as high_fp_frequency
fp_threshold = 10

; Process Analysis
[analysis_module_carbon_black_process_analysis_v1]
module = saq.modules.carbon_black
class = CarbonBlackProcessAnalyzer_v1
enabled = yes
semaphore = carbon_black

[analysis_module_process_guid_analysis]
module = saq.modules.process
class = ProcessGUIDAnalyzer
enabled = yes
semaphore = carbon_black

[analysis_module_dlp_process_hash_analysis_v1]
module = saq.modules.process
class = DLPProcessHashAnalyzer_v1
enabled = yes
semaphore = splunk
max_asset_count = 6
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_bit9_file_hash_analysis_v1]
module = saq.modules.process
class = Bit9FileHashAnalyzer_v1
enabled = no
semaphore = splunk
max_asset_count = 6
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_vt_hash_analyzer]
module = saq.modules.vt
class = VTHashAnalyzer
enabled = yes

; an (optional) comma separated list of vendors to ignore in VT results
ignored_vendors = Tencent,Cylance,eGambit,Endgame,Zillya
; vt_hash_cache url
query_url = OVERRIDE
use_proxy = no

[analysis_module_vt_hash_downloader]
module = saq.modules.vt
class = VTHashFileDownloader
enabled = yes

[analysis_module_alert_correlation]
module = saq.modules.alerts
class = ACEAlertsAnalyzer
enabled = yes

[analysis_module_pcap_extraction]
module = saq.modules.pcap
class = PcapExtraction
enabled = no
semaphore = pcap
; relative time capture (see -d option of the extract script)
relative_duration = 00:01:00
; pcap executable path
executable_path = /opt/ace/bin/extract
; custom pcap extraction configuration path for ACE
config_path = /opt/ace/etc/pcap_extract.ini
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 2

[analysis_module_pcap_conversation_extraction]
module = saq.modules.pcap
class = PcapConversationExtraction
enabled = yes
semaphore = pcap
; relative time capture (see -d option of the extract script)
relative_duration = 00:01:00
; pcap executable path
executable_path = /opt/ace/bin/extract
; custom pcap extraction configuration path for ACE
;
config_path = /opt/ace/etc/pcap_extract.ini
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 2

[analysis_module_network_identifier]
module = saq.modules.asset
class = NetworkIdentifier
enabled = yes
; the CSV file that contains the definitions
csv_file = etc/local_networks.csv

[analysis_module_asset_analyzer]
module = saq.modules.asset
class = AssetAnalyzer
enabled = no

[analysis_module_netbios_analyzer]
module = saq.modules.asset
class = NetBIOSAnalyzer
enabled = yes
; if this is set then the command is prefixed with ssh ssh_host so that it executes from another system
; use this if you are in AWS and your target is inside target network
ssh_host = 

[analysis_module_dns_analyzer]
module = saq.modules.asset
class = DNSAnalyzer
enabled = no
; comma separated list of domains you want to analyze
; this will probably mirror your search list in /etc/resolv.conf
local_domains = 

; if this is set then the command is prefixed with ssh ssh_host so that it executes from another system
; use this if you are in AWS and your target is inside target network
ssh_host = 

[analysis_module_active_directory_analyzer]
module = saq.modules.asset
class = ActiveDirectoryAnalyzer
enabled = yes

[analysis_module_snort]
module = saq.modules.snort
class = SnortAlertsAnalyzer
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
; tighter time around splunk searches
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_elk_snort]
module = saq.modules.elk
class = SnortAlertsAnalyzer
enabled = yes
semaphore = elk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_snort_signature_analysis_v1]
; also see bin/update_snort_rules
module = saq.modules.snort
class = SnortSignatureAnalyzer_v1
enabled = yes
; relative path to snort rules
rules_dir = /opt/ace/etc/snort

[analysis_module_pan_threats]
module = saq.modules.pan
class = PanThreatsAnalyzer
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:proxy
exclude_external = observable_group:external_gateway
exclude_smtp = observable_group:smtp
exclude_internal_dns = observable_group:internal_dns
exclude_external_dns = observable_group:external_dns
; tighter time around splunk searches
relative_duration_before = 04:00:00
relative_duration_after = 04:00:00

[analysis_module_dns_request_analysis_v1]
module = saq.modules.dns
class = DNSRequestAnalyzer_v1
enabled = yes
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 50
; if there are less than X users requesting the resource in the timeframe then we add the users as observables
max_source_count = 6
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00
; 24 hour baseline period
baseline_relative_duration_before = 24:00:00
baseline_relative_duration_after = 02:00:00

[analysis_module_bluecoat_analysis_by_dst_v1]
module = saq.modules.bluecoat
class = BluecoatProxyAnalyzerByDestination_v1
enabled = yes
semaphore = splunk
exclude_proxy = observable_group:internal
; the maximum number of proxy requests to obtain from splunk
max_request_count = 50
; if there are less than X users requesting the resource in the timeframe then we add the users as observables
max_user_count = 6
; tighter time around splunk searches
relative_duration_before = 00:15:00
relative_duration_after = 00:15:00
; 24 hour baseline period
baseline_relative_duration_before = 24:00:00
baseline_relative_duration_after = 02:00:00
; a CSV file that maps bluecoat categories to tags
category_tag_csv_path = etc/bluecoat_category_tagging.csv

[analysis_module_bluecoat_analysis_by_src_v1]
module = saq.modules.bluecoat
class = BluecoatProxyAnalyzerBySource_v1
enabled = yes
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 1000
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00

[analysis_module_squid]
module = saq.modules.squid
class = SquidProxyAnalyzerByDestination
enabled = no
semaphore = splunk
; the maximum number of proxy requests to obtain from splunk
max_request_count = 10
; tighter time around splunk searches
relative_duration_before = 00:15:00
relative_duration_after = 00:15:00

[analysis_module_exploit_kit_proxy_analyzer]
module = saq.modules.bluecoat
class = ExploitKitProxyAnalyzer
enabled = yes
semaphore = splunk
exclude_internal = observable_group:internal
; the maximum number of proxy requests to obtain from splunk
max_request_count = 10
; tighter time around splunk searches
relative_duration_before = 00:01:00
relative_duration_after = 00:01:00

[analysis_module_symantec]
module = saq.modules.symantec
class = SymantecAnalyzer
enabled = yes
semaphore = splunk
; tighter time around splunk searches
relative_duration_before = 24:00:00
relative_duration_after = 00:15:00

[analysis_module_dlp_process]
module = saq.modules.dlp
class = DLPProcessAnalyzer
enabled = yes
semaphore = splunk
; we go a bit wider for DLP Process searches
relative_duration_before = 24:00:00
relative_duration_after = 02:00:00
; go pretty far back for the baseline
baseline_relative_duration_before = 720:00:00
baseline_relative_duration_after = 02:00:00
; this one takes a long time to run so don't go back too far (7 days should be good)
global_baseline_relative_duration_before = 168:00:00
global_baseline_relative_duration_after = 02:00:00

[analysis_module_user_analyzer]
module = saq.modules.user
class = UserAnalyzer
enabled = yes

[analysis_module_email_conversation_frequency_analyzer]
module = saq.modules.email
class = EmailConversationFrequencyAnalyzer
enabled = yes
cooldown_period = 60
; when two people email each other frequently we want to know that
; this is the minimum number of times we've seen this email address email this other email address
; that we consider to be "frequent"
conversation_count_threshold = 5

[analysis_module_email_conversation_attachment_analyzer]
module = saq.modules.email
class = EmailConversationAttachmentAnalyzer
enabled = yes

[analysis_module_email_address_analyzer]
module = saq.modules.user
class = EmailAddressAnalyzer
enabled = yes

[analysis_module_url_email_pivot_analyzer]
module = saq.modules.email
class = URLEmailPivotAnalyzer
enabled = yes
; the maximum amount of results to download
; if there are more emails, only the count is recorded
result_limit = 35

[analysis_module_cloudphish_url_email_pivot_analyzer]
module = saq.modules.email
class = CloudphishURLEmailPivotAnalyzer
enabled = yes

[analysis_module_email_history_analyzer_v2]
module = saq.modules.email
class = EmailHistoryAnalyzer_v2
enabled = yes
semaphore = splunk

; we go back a bit to try to catch phish sitting idle in the inbox
relative_duration_before = 72:00:00
relative_duration_after = 02:00:00

; the following is a list of comma-separated domains that are aliased together
; so if a user's email address domain matches on of these then the entire group is searched with "OR" clause
; for example, john@ashland.com would search for john@teamashland.onmicrosoft.com OR john@ashland.com
; each of these configuration items must start with map_ at the beginning of the name
;map_company_1 = teamcompany.onmicrosoft.com,company.com

[analysis_module_email_logger]
module = saq.modules.email
class = EmailLoggingAnalyzer
enabled = yes

; set this to yes to enable log formatted for splunk
splunk_log_enabled = yes

; the subdirectory inside of splunk_log_dir (see [splunk_logging]) that contains the logs
splunk_log_subdir = smtp
; set this to yes to update the smtplog table of the brocess database
update_brocess = yes

; elasticsearch JSON logging
json_log_enabled = yes

; relative file path of the generated JSON logs
; the file name is passed through strftime 
; https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior
; {pid} is replaced with the process ID of the current executing process 
; so that we don't have multiple processes writing to the same file
json_log_path_format = email/email-{pid}-%%Y%%m%%d.json

[analysis_module_crits_analyzer]
module = saq.modules.crits
class = CritsAnalyzer
enabled = yes

[analysis_module_crits_observable_analyzer]
module = saq.modules.crits
class = CritsObservableAnalyzer
enabled = yes
exclude_common = observable_group:popular_fqdn

[analysis_module_faqueue_alert_analyzer]
module = saq.modules.crits
class = FAQueueAlertAnalyzer
enabled = yes

[analysis_module_vpn_analyzer]
module = saq.modules.vpn_analysis
class = VPNAnalyzer
enabled = yes
semaphore = splunk

; we need to look way back and after to see when they logged in and off
relative_duration_before = 24:00:00
relative_duration_after = 24:00:00

[analysis_module_file_hash_analyzer]
module = saq.modules.file_analysis
class = FileHashAnalyzer
enabled = yes
; ignored file patterns (glob style pattern matching)
ignore_pattern_pcap = *.pcap
ignore_pattern_tshark = *.tshark
ignore_pattern_brotex_1 = *.brotex.tar
ignore_pattern_brotex_2 = *.brotex.tar.gz
ignore_pattern_stderr = *.stderr
ignore_pattern_stdout = *.stdout
ignore_pattern_pdfparser = *.pdfparser
ignore_pattern_stream = *.stream
ignore_pattern_noxml = *.noxml
ignore_pattern_ole = */ole.stdout.analysis
ignore_pattern_officeparser = *.officeparser/*
; not sure why these were in there
;ignore_pattern_olevba = *.olevba/*
;ignore_pattern_vtdownload = */vt_downloads/*
ignore_pattern_brotex_smtp_protocol = */protocol.smtp
ignore_pattern_brotex_http_protocol = */protocol.http
ignore_pattern_brotex_uri_file = */smtp.uriurl.crits
ignore_pattern_brotex_domain_file = */smtp.uridomainname.crits
ignore_pattern_brotex_connection = */connection
;ignore_pattern_email = */email.rfc822
ignore_pattern_email_headers = */email.rfc822.headers
ignore_pattern_email_plain_text = */email.rfc822.unknown_text_plain_*
ignore_mime_type_xml = application/xml
ignore_mime_type_text_xml = text/xml
ignore_mime_type_html = text/html
ignore_mime_type_images = image/*
ignore_mime_type_email = message/rfc822
ignore_pattern_wildfire_reports =  *.wildfire/report.*
ignore_pattern_json = *.json
ignore_pattern_extra_001 = *.crl
ignore_pattern_extra_002 = */xl/drawings/*
ignore_pattern_extra_003 = */xl/media/*
ignore_pattern_extra_004 = */xl/printerSettings/*
ignore_pattern_extra_005 = */xl/theme/*
ignore_pattern_extra_006 = *.rels
ignore_pattern_extra_006 = */xl/vbaProject.bin
ignore_pattern_pdf_txt = *.pdf_txt
ignore_pattern_olevba = *.olevba/*.bas
ignore_pattern_pcode = *.pcode.bas

[analysis_module_correlated_tag_analyzer]
module = saq.modules.tag
class = CorrelatedTagAnalyzer
enabled = yes

; powerpoint autoexec on mouse over to powershell
definition_001_rule = ppt_mouse_over,ppt_program_action,ppt_powershell
definition_001_text = openxml office document has mouseover event leading to powershell script

[analysis_module_ssdeep]
module = saq.modules.file_analysis
class = SsdeepAnalyzer
enabled = yes
; the location of the ssdeep hashes to match against
; generated using ssdeep files_to_hash > etc/ssdeep_hashes
; NOTE - make sure this file doesn't use the characters :
ssdeep_hashes = etc/ssdeep_hashes
; maximum file size (in bytes)
maximum_size = 2048000
; minimum matching threashold (in percent) before the file is tagged as ssdeep
ssdeep_match_threshold = 90

[analysis_module_archive]
module = saq.modules.file_analysis
class = ArchiveAnalyzer
enabled = yes
; comma separated list of excluded mime types we do not want to extract
excluded_mime_types=application/x-shockwave-flash,application/vnd.tcpdump.pcap
; if an archive has more than max_file_count files then we do not analyze it
; more attacks only have a single file inside the zip
; we want to avoid analyzing archives with thousands of files
max_file_count = 6
; the maximum amount of time (in seconds) to wait for 7z to complete
; 7z can go nuts and consume all system memory, so this tries to prevent that
timeout = 5

[analysis_module_olevba_v1_2]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_2
enabled = yes
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 1
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 30

[analysis_module_officeparser_v1_0]
module = saq.modules.file_analysis
class = OfficeParserAnalyzer_v1_0
enabled = yes
; path to the olevba_wrapper script (required)
officeparser_path = /usr/local/bin/officeparser
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with officeparser_failed
timeout = 90

[analysis_module_pcodedmp]
module = saq.modules.file_analysis
class = PCodeAnalyzer
enabled = yes
; full path to the pcodedmp command line utility
pcodedmp_path = /usr/local/bin/pcodedmp

[analysis_module_office_xml_rel]
module = saq.modules.file_analysis
class = OfficeXMLRelationshipExternalURLAnalyzer
enabled = yes

[analysis_module_msoffice_encryption_analyzer]
module = saq.modules.email
class = MSOfficeEncryptionAnalyzer
enabled = yes

; for scanning the email for wordlists
; what is the minimum
range_low = 4
; and maximum size of the passwords to try
range_high = 10
; how many bytes into the email do you want to look?
byte_limit = 1024
; and what is the maximum size of the word list created?
list_limit = 1000

[analysis_module_xml_binary_analyzer]
module = saq.modules.file_analysis
class = XMLBinaryDataAnalyzer
enabled = yes

[analysis_module_xml_plain_text_analyzer]
module = saq.modules.file_analysis
class = XMLPlainTextAnalyzer
enabled = yes

[analysis_module_extracted_ole_analyzer]
module = saq.modules.file_analysis
class = ExtractedOLEAnalyzer
enabled = yes
; comma separated list of things to search for in the output of the file command
suspect_file_type = ms windows shortcut
; comma separated list of extracted file extensions that would be considered suspect here
suspect_file_ext = vbs,jse,exe,jar,lnk,ps1,bat,scr,hta,wsf,cmd,vbe,wsc

[analysis_module_office_file_archiver]
module = saq.modules.file_analysis
class = OfficeFileArchiver
enabled = yes

; the directory (relative to SAQ_HOME) to store the archived office documents
office_archive_dir = archive/office

[analysis_module_file_type]
module = saq.modules.file_analysis
class = FileTypeAnalyzer
enabled = yes

[analysis_module_pdf_analyzer]
module = saq.modules.file_analysis
class = PDFAnalyzer
enabled = yes
; path to pdfparser
pdfparser_path = /opt/ace/bin/pdf-parser.py

[analysis_module_pdftotext]
module = saq.modules.file_analysis
class = PDFTextAnalyzer
enabled = yes
timeout = 5
; path to pdftotext
pdftotext_path = /usr/bin/pdftotext

[analysis_module_yara_scanner_v3_4]
module = saq.modules.file_analysis
class = YaraScanner_v3_4
enabled = yes
; NOTE yara configuration is in the global [yara] section
context_bytes = 64
; amount of time (in minutes) a local scanner stays available
local_scanner_lifetime = 5

[analysis_module_binary_file_analyzer]
module = saq.modules.file_analysis
class = BinaryFileAnalyzer
enabled = yes

[analysis_module_microsoft_script_encoding_analysis]
module = saq.modules.file_analysis
class = MicrosoftScriptEncodingAnalyzer
enabled = yes

; the full path to the program used to decrypt these stupid things
decryption_program = /opt/ace/bin/jscript.decode.pl

[analysis_module_wildfire]
module = saq.modules.wildfire
class = WildfireAnalyzer
enabled = no
frequency = 10
api_key = 2e2c9a2b743ece062e96aa3794ab52c5 
use_proxy = yes

; the total amount of time (in minutes) before we time out a wildfire submission
timeout = 10

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

[analysis_module_cuckoo]
module = saq.modules.cuckoo
class = CuckooAnalyzer
; currently disabled since we have issues with production cuckoo instance (8/25/2016)
enabled = no
use_proxy = no
frequency = 10
timeout = 10
protocol = http
hosts = 

; cuckoo scores files between 0 and 10.0
; scores that exceed this threshold are tagged as malicious
threat_score_threshold = 9.0

# map file extensions to sandbox configurations (work with intel team on this)
#pdf = config1
#jar = config1
#doc = config1
#docx = config1
#docm = config1
#xls = config1
#xlsx = config1
#ppt = config1
#pptx = config1
#rtf = config1
# special default if nothing else matches
default = config1

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

[analysis_module_vxstream_file_analyzer]
module = saq.modules.vx
class = VxStreamFileAnalyzer
enabled = yes

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

[analysis_module_vxstream_hash_analyzer]
module = saq.modules.vx
class = VxStreamHashAnalyzer
enabled = yes

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = no

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

; set to yes to enable downloading memory dumps
download_memory_dumps = no

; (relative) path to configuration file that contains regular expressions of path to avoid
; when processing dropped files
dropped_files_regex_config = etc/vx_dropped.regex

[analysis_module_url_extraction]
module = saq.modules.file_analysis
class = URLExtractionAnalyzer
enabled = yes
; maximum file size in megabytes
max_file_size = 10

[analysis_module_cloudphish]
module = saq.modules.cloudphish
class = CloudphishAnalyzer
enabled = yes

; how long to wait for cloudphish to respond to a request (in seconds)
timeout = 5
; should we use a proxy to access cloudphish?
use_proxy = no
; how often we check on the status of a submission (in seconds)
frequency = 5
; directory to store cached cloudphish query results
local_cache_dir = cloudphish_cache
; how long to wait in total for a URL to be analyed by cloudphish (in seconds)
query_timeout = 300
; instructs remote cloudphish server to generate an ACE alert if it finds something
generate_alert = yes

cloudphish.1 = https://cloudphish1.local:5000/saq/cloudphish

[analysis_module_crawlphish]
module = saq.modules.url
class = CrawlphishAnalyzer
enabled = no

; path to whitelisted netloc
whitelist_path = etc/crawlphish.whitelist
; path to whitelisted path regexes
regex_path = etc/crawlphish.path_regex
; path to blacklisted netloc
blacklist_path = etc/crawlphish.blacklist
; when we are deciding what to crawl we consider if a given hostname (or ip address) is an "uncommon network"
; which means nobody hardly ever goes there
; this value determines the threshold for how many total connections (ever) is considered "yeah we go there"
uncommon_network_threshold = 70
; the user-agent to send with requests
user-agent = Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36
; how long to wait (in full seconds) until a request is timed out
timeout =  8
; maximum file size (in MB)
max_download_size = 10
; how often we reload from crits (in seconds)
crits_refresh_frequency = 300
; how long do wait until we start trying to query brocess again? (in seconds)
cooldown_period = 60
; in some cases you will want to update the brocess database with requests made with crawlphish
; this is true in cases where you do not have http visibility
update_brocess = yes
; comma separate list of proxies to use
; if a request results in a 4** or 5** level error, or errors out, then the next proxy in the list will be attempted
; the special value GLOBAL refers to the global proxy settings (see [proxy])
proxies = GLOBAL

[analysis_module_live_browser_analyzer]
module = saq.modules.url
class = LiveBrowserAnalyzer
enabled = yes

; path to the script that downloads the screenshots from the render server
; TODO get rid of this full path
get_screenshot_path = /opt/ace/bin/get_screenshot
; remote server to be used
remote_server = rendersandbox1.local
; timeout for requests (in seconds)
timeout = 30

[analysis_module_protected_url_analyzer]
module = saq.modules.url
class = ProtectedURLAnalyzer
enabled = yes

[analysis_module_collect_file]
module = saq.modules.collect_file
class = CollectFileAnalyzer
enabled = yes
; how often to check to see if we've collected the file (in seconds)
frequency = 30

[analysis_module_rtfole]
module = saq.modules.file_analysis
class = RTFOLEObjectAnalyzer
enabled = yes
; full path to the rtfobj.py script from oletools package
rtfobj_path = /usr/local/bin/rtfobj
; comma separated list of extensions that are automatically suspect if found inside an RTF OLE object
suspect_ext = js,jse,vbs,vbe,ps1,exe,jar,zip
suspect_mime_type = application/x-dosexec

[analysis_module_extracted_rtf_analyzer]
module = saq.modules.file_analysis
class = ExtractedRTFAnalyzer
enabled = yes
; comma separated list of extensions, mime types and file types that are automatically suspect if found inside an RTF
suspect_ext = js,jse,vbs,vbe,ps1,exe,jar,zip,lnk
suspect_mime_type = application/x-dosexec
suspect_file_type = PE32 executable,MS Windows shortcut

[analysis_module_advanced_link_analyzer]
module = saq.modules.advanced
class = AdvancedLinkAnalyzer
enabled = yes

[analysis_module_message_id_analyzer]
module = saq.modules.email
class = MessageIDAnalyzer
enabled = yes

; how long to wait for a message_id to become available
; this only matters for message_ids with the "delay" directive
; format is HH:MM:SS (default 1 hour)
timeout = 00:10:00

[analysis_module_vbscript_analyzer]
module = saq.modules.file_analysis
class = VBScriptAnalyzer
enabled = yes

; how many characters are considered to be a large hex string
large_hex_string_size = 50

; how many hex strings of size greater than this is considered suspect
large_hex_string_quantity = 50
large_hex_string_quantity_count = 100

; what percentage of the file that is hex string is considered suspect (between 0.0 and 0.99)
hex_string_percentage_limit = 0.15

[analysis_module_nowhitespace_analyzer]
module = saq.modules.file_analysis
class = NoWhiteSpaceAnalyzer
enabled = yes

[analysis_module_meta_refresh_extraction]
; NOTE this is only configured for the cloudphish engine
module = saq.modules.file_analysis
class = MetaRefreshExtractionAnalyzer
enabled = yes

[analysis_module_carbon_black_asset_ident]
module = saq.modules.asset
class = CarbonBlackAssetIdentAnalyzer
enabled = yes

relative_duration_before = 08:00:00
relative_duration_after = 00:15:00

; the maximum number of hostname observables to add
; this is to prevent the case when, for example, 192.168.1.1 is found and you happen to have
; hundreds of devices on some kind of wireless device at home
; if there are more than hostname_limit then we don't add any because you can't really tell which one it was
hostname_limit = 2

;
; DEPRECATED ANALYSIS MODULES
;

# DEPRECATED
[analysis_module_brotex_http_package_analyzer]
module = saq.modules.http
class = BrotexHTTPPackageAnalyzer
enabled = no
; relative path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist
; the maximum amount of requests we'll scan in a single HTTP request
; typically the things we're looking for is a single request, maybe a dozen or so if it's a webpage with images
; set this to 0 to disable (scan everything)
maximum_http_requests = 30

[analysis_module_o365_block_analyzer]
module = saq.modules.email
class = Office365BlockAnalyzer
enabled = no
valid_from_addresses = 

[analysis_module_o365_auto_disposition]
module = saq.modules.email
class = Office365AutoDispositionModule
enabled = no
; the user_id used to disposition the alert
user_id = 
; the minimum number of detections an alert needs to be auto disp
min_detections = 2
; how long to wait for the alerts to show up (in minutes)
timeout = 60
; how often to check for the alerts (in seconds)
frequency = 60

[analysis_module_brotex_smtp_package_analyzer]
module = saq.modules.email
class = BrotexSMTPPackageAnalyzer
enabled = no

[analysis_module_brotex_smtp_stream_archiver]
module = saq.modules.email
class = BrotexSMTPStreamArchiveAction
enabled = no
; the directory to store archives emails (relative to SAQ_HOME)
archive_dir = archive/smtp_stream

[analysis_module_pcap_extraction_service]
module = saq.modules.pcap
class = PcapExtractionServiceAnalyzer
enabled = no
; we don't want to pull pcap for internal systems (too much data)
exclude_internal_network = observable_group:internal
; relative time capture (see -d option of the extract script)
relative_duration = 00:02:00
; the maximum total number of pcap files we should get for a single alert
max_pcap_count = 6
; network location of pcap extraction service
host = 
port = 
; how often do we check to see if the job has completed (in seconds)
check_interval = 5
; how many errors can happen before we stop trying?
error_count_limit = 3

[analysis_module_tshark]
module = saq.modules.pcap
class = TsharkPcapAnalyzer
enabled = no

[analysis_module_bro]
module = saq.modules.pcap
class = BroAnalyzer
enabled = no
; full path to bro binary
bro_bin_path = /nsm/bro/bin/bro
; full path to the ace bro file to run for file extraction
; TODO

[analysis_module_ipdb]
module = saq.modules.ipdb
class = IPDBAnalyzer
enabled = no
csv_file = etc/ip_database.csv
csv_file_encoding = latin-1

[analysis_module_asn]
module = saq.modules.asn
class = ASNAnalyzer
enabled = no
exclude_internal_network = observable_group:internal

netmask_to_asn_file = etc/asn/data-raw-table
netmask_to_asn_file_encoding = latin-1
asn_to_owner_file = etc/asn/data-used-autnums
asn_to_owner_file_encoding = latin-1

[analysis_module_email_history_analyzer_v1]
module = saq.modules.email
class = EmailHistoryAnalyzer_v1
enabled = no
semaphore = splunk
; we go back a bit to try to catch phish sitting idle in the inbox
relative_duration_before = 72:00:00
relative_duration_after = 02:00:00

[analysis_module_mwzoo]
module = saq.modules.file_analysis
class = MalwareZooAnalyzer
enabled = no
mwzoo_home = /opt/mwzoo

[analysis_module_olevba_v1_0]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_0
enabled = no
; path to the olevba_wrapper script (required)
olevba_wrapper_path = bin/olevba_wrapper.py
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 2
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 90

[analysis_module_olevba_v1_1]
module = saq.modules.file_analysis
class = OLEVBA_Analyzer_v1_1
enabled = no
; path to the olevba_wrapper script (required)
olevba_wrapper_path = bin/olevba_wrapper.py
; define the minimum thresholds required for a file to be tagged (alerted)
; these are the "analysis types" as defined here (https://bitbucket.org/decalage/oletools/wiki/olevba)
; format is threshold_analysis_type where analysis_type is lowercase name of analysis type (Type column in olevba.py output table)
threshold_autoexec = 1
;threshold_ioc = 1
threshold_suspicious = 2
; amount of time to wait for the process to finish (in seconds)
; if the process doesn't finish then we tag the file with olevba_failed
timeout = 30

[analysis_module_ole_archiver_v1_0]
module = saq.modules.file_analysis
class = OLEArchiver_v1_0
enabled = no
; directory to contain all the archived office documents
ole_archive_dir = archive/ole

[analysis_module_vxstream_v1_0]
module = saq.modules.vx
class = VxStreamAnalyzer_v1_0
enabled = no
expect_proxy = 

; the total amount of time (in minutes) before we time out a vxstream submission
timeout = 20
; the amount of time between status checks (in seconds)
frequency = 10
; set this to true to use the proxy configured in [proxy]
use_proxy = yes

; comma separated list of supported file extensions
; there is also support for checking for things like MZ, OLE and PDF headers built into the module
supported_extensions = doc,docx,docm,xls,xlsx,xlsm,ppt,pptx,pptm,pdf,js,vbs,jse,exe,dll,swf,jar,lnk,ps1,rtf,chm,bat,scr,hta,cab,pif,au3,a3x,eps,xla,pptm,pps,dot,dotm,pub,wsf,cmd,ps,vbe,wsc

; thresholds for generating alerts
; samples with a "score" of this or higher will alert
threat_score_threshold = 100
; sampels with a "threat level" of this or higher will alert
threat_level_threshold = 2

;
; PROFILE POINT ANALYSIS
;

[profile_points]
; a comma separated list of modules that contain profile point classes to load
modules = saq.modules.profilepoint
; the minimum threshold (in percent range 0 to 100) that determines if a profile point is displayed as "matched"
display_threshold = 35
; a comma separated list of profile point classes to disable
disabled_profile_points = 

;
; SITE MODULES
;

;
; TAGGING MODULES
;

; tag people in specific groups in the organziation
[analysis_module_user_tagger]
module = saq.modules.user
class = UserTaggingAnalyzer
enabled = yes

; location of the cache (use saq update-organization to build this file)
json_path = etc/organization.json

; FORMAT: group_GROUPNAME = BOSS,LEVELS_DEEP,TAG
; GROUPNAME = some unique name to give the group
; BOSS = the person at the organzation leading the group
; LEVELS_DEEP = how far deep in direct reports to include people in the group
; TAG = what tag to give people in this group

; CEO and direct reports
; group_ceo - userid,2,executive

[analysis_module_site_tagger]
module = saq.modules.tag
class = SiteTagAnalyzer
enabled = yes
;
; the CSV file that contains the tagging definitions
; format is as follows
; o_types,match_type,ignore_case,value,tags
; o_types : list of valid observable types separated by | (pipe) character
; match_type: one of the following values
;
; TYPE          DESCRIPTION
; default       default string matching (exact match)
; glob          unix-style glob pattern matching
; regex         regular expression
; cidr          CIDR network matching
; subdomain     subdomain matching
;
; ignore_case : boolean (see https://docs.python.org/3.4/library/stdtypes.html#truth) to ignore case or not
; value : the value to match against
; tags : list of tags separated by | (pipe) character to apply if observable matches
;
csv_file = etc/site_tags.csv

[analysis_module_tag_email_notification]
module = saq.modules.tag
class = EmailNotification
enabled = no

smtp_mail_from = 
smtp_subject_prefix =
smtp_server =

; each configuration option starts with tag_distro_
; the value is tags_list_csv:email_list_csv
; tags_list_csv is a comma separated list of tags that MUST be present for a match
; email_list_csv is a comma separated list of email addresses
tag_distro_vioc = critical:cybersecurity@localhost

[disabled_modules]
; zero or more analysis module section names that should be disabled
; this is for configurations that require that certain modules not run
; for example: an environment where samples should not be submitted to sandboxes

analysis_module_wildfire = yes
analysis_module_ole_archiver_v1_0 = yes
analysis_module_olevba_v1_0 = yes
analysis_module_olevba_v1_1 = yes

;
; TAG PRIORITIES AND SETTINGS
;

[tags]

; valid values are
; hidden
; special (white)
; fp (green)
; info  (gray)
; warning (yellow)
; alert (red)
; critical (red)

; defaults to info if the tag isn't in this list
; warning and above trigger alerts

mail_to = hidden
delivered_to = hidden
mail_from = hidden
reply_to = hidden
no_render = hidden

high_fp_frequency = special
whitelisted = special
encrypted_email = special
decrypted_email = special

debug = fp
qualys = fp
infosec = fp
p2p = fp
frequent_conversation = fp
guest_wireless = fp

new_sender = interesting

treasury = warning
suspicious_url  = warning
suspect = warning
olevba = warning
;olevba_failed = warning
officeparser_failed = warning
rtf_binary = warning
unexpected_binary_data = warning
microsoft_script_encoding = warning

office365_block = alert
pos = alert
vioc = alert
phish = alert
executive = alert
apt = alert
av = alert
dc = alert
malicious_url = alert
malicious = alert
ssdeep = alert
rat = alert
script_in_zip = alert
exe_in_zip = alert
lnk_in_zip = alert
suspect_ole_attachment = alert
;high_mal_frequency = alert

critical = critical

[tag_css_class]

; css classes to use for the various levels
hidden =
special = label-warning label-special
fp = label-success
info = label-default
warning = label-warning
interesting = label-warning label-interesting
alert = label-danger
critical = label-danger

;
; MODULE GROUPS
;

[module_group_correlation]
; all the modules you'd want minus any special modules designed for specific systems
analysis_module_active_directory_analyzer = yes
analysis_module_advanced_link_analyzer = yes
analysis_module_alert_correlation = yes
analysis_module_bluecoat_analysis_by_dst_v1 = yes
analysis_module_bluecoat_analysis_by_src_v1 = yes
analysis_module_carbon_black_asset_ident = yes
analysis_module_carbon_black_process_analysis_v1 = yes
analysis_module_cloudphish = yes
analysis_module_collect_file = yes
analysis_module_crits_observable_analyzer = yes
analysis_module_dlp_process = yes
analysis_module_dlp_process_hash_analysis_v1 = yes
analysis_module_dns_request_analysis_v1 = yes
analysis_module_email_address_analyzer = yes
analysis_module_email_history_analyzer_v1 = yes
analysis_module_email_history_analyzer_v2 = yes
analysis_module_exploit_kit_proxy_analyzer = yes
analysis_module_hal9000 = yes
analysis_module_live_browser_analyzer = yes
analysis_module_netbios_analyzer = yes
analysis_module_network_identifier = yes
analysis_module_pan_threats = yes
analysis_module_pcap_conversation_extraction = yes
analysis_module_process_guid_analysis = yes
analysis_module_elk_snort = yes
analysis_module_snort = yes
analysis_module_snort_signature_analysis_v1 = yes
analysis_module_symantec = yes
analysis_module_cloudphish_url_email_pivot_analyzer = yes
analysis_module_url_email_pivot_analyzer = yes
analysis_module_user_analyzer = yes
analysis_module_vpn_analyzer = yes
analysis_module_vt_hash_analyzer = yes
analysis_module_vt_hash_downloader = yes

[module_group_file]
; everything you would need for file analysis
analysis_module_archive = yes
analysis_module_binary_file_analyzer = yes
analysis_module_collect_file = yes
analysis_module_extracted_ole_analyzer = yes
analysis_module_extracted_rtf_analyzer = yes
analysis_module_file_hash_analyzer = yes
analysis_module_file_path_analysis = yes
analysis_module_file_type = yes
analysis_module_microsoft_script_encoding_analysis = yes
analysis_module_nowhitespace_analyzer = yes
analysis_module_officeparser_v1_0 = yes
analysis_module_ole_archiver_v1_0 = yes
analysis_module_olevba_v1_2 = yes
analysis_module_pcodedmp = yes
analysis_module_pdf_analyzer = yes
analysis_module_pdftotext = yes
analysis_module_rtfole = yes
analysis_module_site_tagger = yes
analysis_module_ssdeep = yes
analysis_module_url_extraction = yes
analysis_module_vbscript_analyzer = yes
analysis_module_vxstream_file_analyzer = yes
analysis_module_vxstream_hash_analyzer = yes
analysis_module_wildfire = yes
analysis_module_xml_binary_analyzer = yes
analysis_module_xml_plain_text_analyzer = yes
analysis_module_yara_scanner_v3_4 = yes

[module_group_email]
; everything related to email scanning
analysis_module_email_analyzer = yes
analysis_module_email_conversation_attachment_analyzer = yes
analysis_module_email_conversation_frequency_analyzer = yes
analysis_module_email_link_analyzer = yes
analysis_module_encrypted_archive_analyzer = yes
analysis_module_message_id_analyzer = yes
analysis_module_msoffice_encryption_analyzer = yes
;analysis_module_o365_block_analyzer = yes
analysis_module_smtp_stream_analyzer = yes

[module_group_common]
analysis_module_cloudphish = yes
analysis_module_correlated_tag_analyzer = yes
analysis_module_crits_analyzer = yes
analysis_module_site_tagger = yes
analysis_module_user_tagger = yes

;
; ENGINE CONFIGURATIONS
;

; alert correlation engine
[engine_ace]
enabled = no
profile_points_enabled = no
module_groups = common,correlation,file,email
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new binaries (in seconds)
collection_frequency = 1
; how often to dump statistics (in seconds)
statistic_dump_frequency = 3
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; the host and port we listen on for the network connections from the ace clients
server_host = 0.0.0.0
server_port = 12343
; the paths to the SSL keys used for communication
; NOTE the cert is PEM encoded

ssl_cert_path = OVERRIDE
ssl_key_path = OVERRIDE
ssl_ca_path = ssl/ca-chain.cert.pem

; amount of time until a network connection is considered timed out (in seconds)
network_timeout = 30
; the maximum number of simultaneous connections
max_connections = 5

; matches the name column in the database_workload database
workload_name = ACE

; the correlation engine is the only engine that looks at these alerts
analysis_module_faqueue_alert_analyzer = yes

; ----------------------------------------------------------------------------

[engine_carbon_black]
enabled = no
profile_points_enabled = no
module_groups = common,file
; general engine configuration
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new binaries (in seconds)
collection_frequency = 10
; how often to dump statistics (in seconds)
statistic_dump_frequency = 3
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; Carbon Black specific configuration
;
; carbon black server API location and authentication
url = 
token = 
; the storage directory for downloaded files
storage_dir = storage
; how many binaries to download at once
download_batch_size = 10
; when starting up, how far back do we look for new binaries? (in hours)
; set to 0 to initially get ALL available binaries
initial_search_offset = 24
; how far back to check for new binarys (in minutes)
search_offset = 60

; ----------------------------------------------------------------------------

[engine_orion]
enabled = no
profile_points_enabled = no
module_groups = common,file
; general engine configuration
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new office documents (in seconds)
collection_frequency = 10
; how often to dump statistics (in seconds)
statistic_dump_frequency = 3
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; orion specific configuration
;
; carbon black server API location and authentication
url = 
token = 
; how many filelocations to locate at once
download_batch_size = 10
; when starting up, how far back do we look for new file locations? (in hours)
; set to 0 to initially get ALL available binaries
initial_search_offset = 24
; how far back to check for new binarys (in minutes)
search_offset = 60
; relative file name for local shelve database to keep track of what was already downloaded
tracking_db = tracking.db

analysis_module_collect_file = yes

; ----------------------------------------------------------------------------

; configuration for command line correlation
[engine_cli_correlation]
enabled = yes
profile_points_enabled = no
module_groups = common,correlation,file,email
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = 2
; how often to check for new binaries (in seconds)
collection_frequency = 1
; how often to dump statistics (in seconds)
statistic_dump_frequency = 0
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; ----------------------------------------------------------------------------

; configuration for the brotex stream engine
[engine_brotex_stream]
enabled = = no
profile_points_enabled = no
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new binaries (in seconds)
collection_frequency = 1
; how often to dump statistics (in seconds)
statistic_dump_frequency = 1
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; the host and port we listen on for the network connections from the brotex clients
server_host = 0.0.0.0
server_port = 18930
; the paths to the SSL keys used for brotex communication
; NOTE the cert is PEM encoded
ssl_cert_path = OVERRIDE
ssl_key_path = OVERRIDE
; amount of time until a network connection is considered timed out (in seconds)
network_timeout = 30
; the maximum number of simultaneous connections
max_connections = 5

; matches the name column in the database_workload database
workload_name = BROTEX

; we only enable a few modules for this engine
analysis_module_brotex_http_package_analyzer = yes
analysis_module_brotex_smtp_package_analyzer = yes
analysis_module_brotex_smtp_stream_archiver = yes
analysis_module_file_type = yes
analysis_module_smtp_stream_analyzer = yes

; ----------------------------------------------------------------------------

; configuration for email scanner
[engine_email_scanner]
enabled = no
profile_points_enabled = no
module_groups = common,file,email
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new binaries (in seconds)
collection_frequency = 1
; how often to dump statistics (in seconds)
statistic_dump_frequency = 1
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 300
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; matches the name column in the database_workload database
workload_name = EMAIL
; amount of time until a network connection is considered timed out (in seconds)
network_timeout = 30
; the maximum number of simultaneous connections
max_connections = 5

; the host and port we listen on for the network connections from the ace mailbox clients
server_host = 0.0.0.0
server_port = 12344
; the paths to the SSL keys used for brotex communication
; NOTE the cert is PEM encoded
ssl_cert_path = OVERRIDE
ssl_key_path = OVERRIDE
ssl_ca_path = ssl/ca-chain.cert.pem

; directory (relative to SAQ_HOME) that contains the smtp files extracted from bro sensors
; this directory is populated by either a local bro instance or by a remote bro instance sending via API
bro_smtp_dir = var/bro/smtp

; the email scanner is the only enigne that archives emails and logs to splunk
analysis_module_email_archiver = yes
analysis_module_email_logger = yes
; we do want to use vt for detection here
analysis_module_vt_hash_analyzer = yes
; keep an archive of office files for analysis work
analysis_module_office_file_archiver = yes
; required by office_file_archiver
analysis_module_file_type = yes

; ----------------------------------------------------------------------------

; configuration for http scanner
[engine_http_scanner]
enabled = no
; for anp-enabled engines, what mode the engine runs in
; valid values are local, server and client
mode = local

; maximum workload size before we start reporting BUSY to anp clients
anp_workload_max_size = 1024

anp_listening_address = 127.0.0.1
anp_listening_port = 41433

profile_points_enabled = no
module_groups = common,file
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new binaries (in seconds)
collection_frequency = 1
; how often to dump statistics (in seconds)
statistic_dump_frequency = 1
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; location of the incoming bro http streams, relative to SAQ_HOME
bro_http_dir = var/bro/http
; path to the brotex custom whitelist file
whitelist_path = etc/brotex.whitelist

; matches the name column in the database_workload database
workload_name = HTTP
; don't do any cloudphish analysis here (since we're already scanning http traffic)
analysis_module_cloudphish = no
; keep an archive of office files for analysis work
analysis_module_office_file_archiver = yes
; required by office_file_archiver
analysis_module_file_type = yes

; ----------------------------------------------------------------------------

[cloudphish]
; the location of cached data downloaded by the cloudphish engine
cache_dir = cloudphish

[engine_cloudphish]
enabled = no
profile_points_enabled = no
module_groups = common,file
; the maximum number of parralel analysis jobs at a time
analysis_pool_size = OVERRIDE
; how often to check for new work
collection_frequency = 0
; how often to dump statistics (in seconds)
statistic_dump_frequency = 3
; how often to potentially reload modules (in seconds)
auto_reload_frequency = 5
; how often to refresh the process managers (in seconds)
auto_refresh_frequency = 0

; the local host name we use as the "location" of the content
; this is typically the FQDN of the local host
location = OVERRIDE

analysis_module_crawlphish = yes
analysis_module_protected_url_analyzer = yes
analysis_module_cloudphish = no
analysis_module_user_tagger = no
analysis_module_meta_refresh_extraction = yes

